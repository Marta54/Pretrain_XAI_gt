{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29790a5",
   "metadata": {},
   "source": [
    "# Imagenet results\n",
    "\n",
    "+ classification performance metrics\n",
    "     + AUROC\n",
    "     + AUPRC\n",
    "     + Accuracy\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7e97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "from torchmetrics.classification import BinaryAveragePrecision\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "from torchmetrics.classification import MulticlassAUROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85e0371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "torchmetrics.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55feef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import transform\n",
    "from skimage.transform import resize\n",
    "import skimage.exposure as skie\n",
    "\n",
    "import ot\n",
    "\n",
    "import torch\n",
    "from torch import manual_seed\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import Image \n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.set_num_threads(3)\n",
    "\n",
    "def compactness(blob_labels):\n",
    "    import math\n",
    "    compactness=[]\n",
    "    region=regionprops(blob_labels)\n",
    "    for rp in region:\n",
    "        area=rp.area\n",
    "        #perimeter=rp.perimeter\n",
    "        perimeter=rp.perimeter_crofton\n",
    "        c=(4*math.pi*area)/(perimeter**2)\n",
    "        compactness.append(c)\n",
    "    return compactness\n",
    "\n",
    "def get_blobs(image,lesion_size):\n",
    "    \"\"\"\n",
    "    gets the noise image after pre processing and returns blobs with the size equal to lesion size\n",
    "    image is the noise image \n",
    "    lesion_size can be an int\n",
    "    \"\"\"\n",
    "    \n",
    "    labeled, nr_objects = ndimage.label(image)\n",
    "    sizes = ndimage.sum_labels(image,labeled,range(nr_objects+1))    \n",
    "    mask_size=sizes!=lesion_size\n",
    "    small_blobs=labeled.copy()\n",
    "    remove_pixel = mask_size[small_blobs]\n",
    "    small_blobs[remove_pixel] = 0 \n",
    "    \n",
    "    return small_blobs\n",
    "\n",
    "\n",
    "def list_lesions(image,lesion_size):\n",
    "    round_lesion_c=0.8 #lesions with compactness above this value are considered round\n",
    "    not_round_lesion_c=0.4 #lesions with compactness bellow this value are considered not round\n",
    "    \n",
    "    border=2\n",
    "    blur=0.75\n",
    "    \n",
    "    less_size=lesion_size*0.05*blur #the lesion is smaller after the blur\n",
    "    \n",
    "    small_blobs=get_blobs(image,lesion_size)\n",
    "    rb=regionprops(small_blobs)\n",
    "    round_lesions=[]\n",
    "    not_round_lesions=[]\n",
    "    c_list=compactness(small_blobs)\n",
    "    \n",
    "    for blob in range(len(c_list)):\n",
    "        \n",
    "        if c_list[blob]>round_lesion_c:#round lesions\n",
    "\n",
    "            blob_img=rb[blob].image.astype(float)\n",
    "            #the image is padded because when it is smoothed it increases a bit\n",
    "            pad_img=np.pad(array=blob_img, pad_width=border, mode='constant', constant_values=0)\n",
    "            blur_image=ndimage.gaussian_filter(pad_img, blur)\n",
    "            blur_image[blur_image<0.2]=0\n",
    "            energy=round(np.sum(blur_image.astype(float)),2)\n",
    "            if energy<=lesion_size-less_size+1 and energy>=lesion_size-less_size-1:\n",
    "                round_lesions.append([blur_image,round(c_list[blob],3)])\n",
    "        \n",
    "        \n",
    "        elif c_list[blob]<not_round_lesion_c:# not round lesions\n",
    "            blob_img=rb[blob].image.astype(float)\n",
    "            #the image is padded because when it is smoothed it increases a bit\n",
    "            pad_img=np.pad(array=blob_img, pad_width=border, mode='constant', constant_values=0)\n",
    "            blur_image=ndimage.gaussian_filter(pad_img, blur)\n",
    "            blur_image[blur_image<0.15]=0\n",
    "            energy=round(np.sum(blur_image.astype(float)),2)\n",
    "            if energy<=lesion_size-less_size+1 and energy>=lesion_size-less_size-1:\n",
    "                not_round_lesions.append([blur_image,round(c_list[blob],3)])\n",
    "            \n",
    "    return round_lesions, not_round_lesions\n",
    "\n",
    "def create_lesions(lesion_number,lesion_size,factor=1):\n",
    "    #factor is the value for which we multiply the sides of the lesion\n",
    "    round_lesions=[]\n",
    "    not_round_lesions=[]\n",
    "    s=0\n",
    "    size_noise=256\n",
    "    blur_radius=2\n",
    "\n",
    "    while len(not_round_lesions)<=lesion_number or len(round_lesions)<=lesion_number:\n",
    "        #create noise\n",
    "        np.random.seed(s)\n",
    "        noise_img=np.random.rand(size_noise,size_noise)\n",
    "\n",
    "        #smooth noise\n",
    "        imgf=ndimage.gaussian_filter(noise_img, blur_radius)\n",
    "\n",
    "        #create binary image\n",
    "        thr=threshold_otsu(imgf)\n",
    "        imgf_bin=imgf>thr\n",
    "        \n",
    "        #morphologic changes\n",
    "        erosion_image=ndimage.binary_erosion(imgf_bin)\n",
    "        open_er_img=ndimage.binary_opening(erosion_image)\n",
    "        erosion_image2=ndimage.binary_erosion(open_er_img)\n",
    "\n",
    "        #images\n",
    "        \n",
    "\n",
    "        #as a result from one noise image we create several images that can be used to create the lesions\n",
    "        round_lesions_open, not_round_lesions_open=list_lesions(open_er_img,lesion_size)\n",
    "        round_lesions_er, not_round_lesions_er=list_lesions(erosion_image2,lesion_size)\n",
    "        \n",
    "        round_lesions=round_lesions+round_lesions_open+round_lesions_er\n",
    "        not_round_lesions=not_round_lesions+not_round_lesions_open+not_round_lesions_er\n",
    "        \n",
    "        \n",
    "        #print('len lists:',len(round_lesions),len(not_round_lesions))\n",
    "        '''        \n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.subplot(2,4,1)\n",
    "        plt.imshow(noise_img)\n",
    "        plt.title('noise')\n",
    "        plt.subplot(2,4,2)\n",
    "        plt.imshow(imgf)\n",
    "        plt.title('blur')\n",
    "        plt.subplot(2,4,3)\n",
    "        plt.imshow(imgf_bin)\n",
    "        plt.title('binary img')\n",
    "        plt.subplot(2,4,4)\n",
    "        plt.imshow(erosion_image)\n",
    "        plt.title('erosion')\n",
    "        plt.subplot(2,4,5)\n",
    "        plt.imshow(open_er_img)\n",
    "        plt.title('open')\n",
    "        plt.subplot(2,4,6)\n",
    "        plt.imshow(erosion_image2)\n",
    "        plt.title('erosion2')\n",
    "        plt.subplot(2,4,7)\n",
    "        plt.imshow(dilated_image)\n",
    "        plt.title('dilation')\n",
    "        break\n",
    "        '''\n",
    "        \n",
    "        s+=1\n",
    "    print(f'round lesions: {len(round_lesions)}/{lesion_number} === not round lesions {len(not_round_lesions)}/{lesion_number}')\n",
    "    print(f'number of seeds used: {s-1}')\n",
    "    print(f'the lesions have size between {lesion_size-lesion_size*0.05*0.75-1} and {lesion_size-lesion_size*0.05*0.75+1}')\n",
    "    \n",
    "    lesions_r=round_lesions[:lesion_number]\n",
    "    lesions_nr=not_round_lesions[:lesion_number]\n",
    "    round_lesions=[[np.array(resize(round_lesions[i][0],(round_lesions[i][0].shape[0]*factor,round_lesions[i][0].shape[1]*factor))),round_lesions[i][1]] for i in range(len(lesions_r))]\n",
    "    not_round_lesions=[[np.array(resize(not_round_lesions[i][0],(not_round_lesions[i][0].shape[0]*factor,not_round_lesions[i][0].shape[1]*factor))),not_round_lesions[i][1]] for i in range(len(lesions_nr))]\n",
    "    \n",
    "    return round_lesions, not_round_lesions\n",
    "\n",
    "def rescale_values(image,max_val,min_val):\n",
    "    '''\n",
    "    image - numpy array\n",
    "    max_val/min_val - float\n",
    "    '''\n",
    "    return (image-image.min())/(image.max()-image.min())*(max_val-min_val)+min_val\n",
    "\n",
    "def select_coordinates(slice_image, lesions,white_constant,seed):\n",
    "    '''\n",
    "    slice_image is the brain slice to use\n",
    "    lesions is a list of the lesions (with len=number_of_lesions) to use\n",
    "    colour_lesion is either 'black' or 'white'\n",
    "    white_constant is the constant that is multiplied with the lesion mask to create lighter or darker lesions\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    brain_mask=np.array(slice_image)>0\n",
    "    brain_mask=brain_mask.astype(float)\n",
    "    x,y = np.where(brain_mask == 1.)\n",
    "    \n",
    "    lesion_brain=slice_image.copy().astype(float)\n",
    "    lesion_mask=brain_mask.copy() \n",
    "    lesion_added=0\n",
    "    ground_truth=np.zeros(slice_image.shape)\n",
    "    min_value=0.1 #min value for the lesion intensity \n",
    "    max_value=0.9 #max value for the lesion intensity\n",
    "    brain_image=slice_image.copy()\n",
    "    \n",
    "    while lesion_added<len(lesions):\n",
    "        i=np.random.choice(np.arange(len(x)))\n",
    "        coordinate=[x[i],y[i]]\n",
    "        lesion=lesions[lesion_added]\n",
    "        lesion_rescale = rescale_values(lesion,max_value,min_value)\n",
    "        lesion_rescale=rescale_values(lesion,white_constant,min_value)\n",
    "        \n",
    "        #creating the lesion mask and ground truth\n",
    "        if (brain_mask[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]] ==1).all():\n",
    "            #checks if the lesion that will be added is completly in a white space of the lesion mask \n",
    "            #(this means that the new lesion is not overlaping an existing one and is completly in the brain area)\n",
    "            lesion_mask[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]] -= lesion_rescale\n",
    "            lesion_added+=1\n",
    "            ground_truth[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]]+=lesion\n",
    "            brain_mask=lesion_mask\n",
    "\n",
    "    \n",
    "    brain_image=slice_image.copy()\n",
    "    brain_mask=np.array(slice_image)>0\n",
    "    #creating the white lesions\n",
    "       \n",
    "    brain_image[brain_image>0]=1-brain_image[brain_image>0]        \n",
    "    brain_image[lesion_mask!=0]*=lesion_mask[lesion_mask!=0]\n",
    "    brain_image[brain_mask]=1-brain_image[brain_mask]\n",
    "    \n",
    "\n",
    "    \n",
    "    return lesion_mask,brain_image,ground_truth\n",
    "    \n",
    "def add_lesions(slice_image,round_lesions,not_round_lesions,min_lesion,max_lesion,white_constant,seed,max_brain):\n",
    "    #for each slice we chose: random number of lesions, random lesions, random coordinates\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    number_of_lesions=np.random.randint(min_lesion,max_lesion+1)\n",
    "    lesion_type=np.random.randint(0,2)\n",
    "    #lesion_type=0 - round\n",
    "    #lesion_type=1 - not round\n",
    "    \n",
    "    #get lesions from type of lesions (and target)\n",
    "    added=rescale_values(slice_image.copy(),max_brain,0)\n",
    "    if lesion_type==0: #round\n",
    "        target=0\n",
    "        with open('round_lesions.pkl', 'rb') as f:\n",
    "            lesion_list=pickle.load(f)\n",
    "        np.random.shuffle(round_lesions)\n",
    "        \n",
    "    elif lesion_type==1: #not round\n",
    "        target=1\n",
    "        with open('not_round_lesions.pkl', 'rb') as f:\n",
    "            lesion_list=pickle.load(f) \n",
    "        np.random.shuffle(lesion_list)\n",
    "        \n",
    "    lesions=[i[0] for i in lesion_list[:number_of_lesions]]\n",
    "    #add the lesions\n",
    "    \n",
    "    lesion_mask,lesion_brain_white,ground_truth=select_coordinates(added, lesions,white_constant,seed)\n",
    "    \n",
    "    \n",
    "    return lesion_mask,lesion_brain_white,ground_truth,target,number_of_lesions\n",
    "\n",
    "def change_images(image):\n",
    "    image=np.repeat(image[..., np.newaxis], 3, axis=2)\n",
    "    image=resize(image, (224, 224))\n",
    "    image=image.transpose(2,0,1)\n",
    "    return image\n",
    "\n",
    "def create_dataset(slices,round_lesions,not_round_lesions,min_lesion=3,max_lesion=5,white_constant=0.85,seed=0,max_brain=1):\n",
    "\n",
    "    dataset_white=[]\n",
    "    number_lesions=[]\n",
    "    lesion_mask_list=[]\n",
    "    ground_truths=[]\n",
    "    for slice_idx in range(len(slices)):\n",
    "        lesion_mask,lesion_brain_white,ground_truth,target,number_of_lesions=add_lesions(slices[slice_idx],\n",
    "                                                                                         round_lesions,\n",
    "                                                                                         not_round_lesions,\n",
    "                                                                                         min_lesion=min_lesion,\n",
    "                                                                                         max_lesion=max_lesion,\n",
    "                                                                                         white_constant=white_constant,\n",
    "                                                                                        seed=seed,\n",
    "                                                                                        max_brain=max_brain)\n",
    "        dataset_white.append([change_images(lesion_brain_white),target])\n",
    "        number_lesions.append(number_of_lesions)\n",
    "        lesion_mask_list.append(lesion_mask)\n",
    "        ground_truths.append(ground_truth)\n",
    "        seed+=1\n",
    "        \n",
    "        if slice_idx%1500==0:\n",
    "            print(f'slice {slice_idx}/{len(slices)} = {round(100*slice_idx/len(slices),2)}%')\n",
    "        \n",
    "    return dataset_white,number_lesions,lesion_mask_list,ground_truths\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d2ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VGG_model(path,device):\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.classifier=model.classifier[:-1]\n",
    "    last_layers=[nn.Linear(4096,2)]\n",
    "    model.classifier = nn.Sequential(*list(model.classifier)+last_layers) \n",
    "\n",
    "    model.load_state_dict(torch.load(path,map_location=device))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724132f",
   "metadata": {},
   "source": [
    "### Best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be452fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_labels(model, dataset, DEVICE):\n",
    "    BATCH = 32\n",
    "    dataloader = DataLoader(dataset,batch_size=BATCH)\n",
    "\n",
    "    real_labels = []\n",
    "    pred_labels = []\n",
    "    logits_list = []\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "\n",
    "        inputs = inputs.to(DEVICE,dtype=torch.float)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        real_labels += torch.Tensor.cpu(labels).tolist()\n",
    "        \n",
    "        logits_list += torch.Tensor.cpu(logits).tolist()\n",
    "        \n",
    "        pred_labels += torch.Tensor.cpu(torch.max(logits, 1)[1]).tolist()\n",
    "        \n",
    "    model=None\n",
    "    inputs=None\n",
    "    labels=None\n",
    "    logits=None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return real_labels, pred_labels, logits_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "117b1692",
   "metadata": {},
   "outputs": [],
   "source": [
    " model=None\n",
    "inputs=None\n",
    "labels=None\n",
    "logits=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79abf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_performance(logits, target):\n",
    "#     metric = BinaryAUROC(thresholds=None)\n",
    "#     AUROC = metric(logits, target)\n",
    "    \n",
    "#     metric = BinaryAveragePrecision(thresholds=None)\n",
    "#     AUPRC = metric(logits, target) # preds are logits  \n",
    "    \n",
    "#     metric = BinaryAccuracy()\n",
    "#     ACC = metric(logits, target)\n",
    "\n",
    "    metric = MulticlassAveragePrecision(num_classes=2)\n",
    "    AUPRC = metric(logits, target)\n",
    "\n",
    "    metric = MulticlassAccuracy(num_classes=2)\n",
    "    ACC = metric(logits, target)\n",
    "\n",
    "    metric = MulticlassAUROC(num_classes=2)\n",
    "    AUROC = metric(logits, target)\n",
    "\n",
    "    return AUROC, AUPRC, ACC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4173e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round lesions: 51/50 === not round lesions 207/50\n",
      "number of seeds used: 886\n",
      "the lesions have size between 66.375 and 68.375\n",
      "took 10.5s\n",
      " ====== holdout ====== \n",
      "slice 0/8539 = 0.0%\n",
      "slice 1500/8539 = 17.57%\n",
      "slice 3000/8539 = 35.13%\n",
      "slice 4500/8539 = 52.7%\n",
      "slice 6000/8539 = 70.27%\n",
      "slice 7500/8539 = 87.83%\n",
      "\n",
      "took 153.08s\n",
      "4277 slices of target 1 out of 8539 slices: 50.09 %\n",
      " number of slices: 8539\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3412019/452030037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' number of slices: {len(dataset)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'total time: {(time.time()-t0) // 60:.0f}m {(time.time()-t0) % 60:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't0' is not defined"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "plt.rc('image',cmap='gray')  \n",
    "\n",
    "# creating lesions\n",
    "number_of_lesions=50 #amount of lesions in each lesion list\n",
    "size_of_lesions=70 #size of all the lesions\n",
    "factor=2\n",
    "\n",
    "round_lesions, not_round_lesions=create_lesions(number_of_lesions,size_of_lesions,factor=factor)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "\n",
    "# loading slices \n",
    "with open('slices_validation.pkl', 'rb') as f:\n",
    "    validation_slices,target_valid_gender,target_valid_age = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# adding lesions to slices\n",
    "lesion_max_intensity=0.5\n",
    "max_brain_intensity=0.7\n",
    "\n",
    "start = time.time()\n",
    "print(' ====== holdout ====== ')\n",
    "\n",
    "dataset,_,_,ground_truths=create_dataset(validation_slices,\n",
    "                                          round_lesions,\n",
    "                                          not_round_lesions,\n",
    "                                          min_lesion=3,\n",
    "                                          max_lesion=5,\n",
    "                                          white_constant=lesion_max_intensity,\n",
    "                                          seed=50000,\n",
    "                                          max_brain=max_brain_intensity)\n",
    "\n",
    "target_w=[i[1] for i in dataset]\n",
    "print(f'{len([i for i in target_w if i==1])} slices of target 1 out of {len(target_w)} slices: {round(100*len([i for i in target_w if i==1])/len(target_w),2)} %')\n",
    "print(f' number of slices: {len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe1b6de7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_class_metrics(folder, models_names, dataset, output_file_path, device = 'cuda:0'):\n",
    "    # saves the real and predicted labels, the logits and the three classification metrics\n",
    "    # based on each of the models in model_names\n",
    "    #\n",
    "    # folder is the path to the folder where the models are saved\n",
    "    # model_names is a list of strings that correspond to the name of the file of the model\n",
    "    # output_file_name is the string that corresponds to the folder where the list of dictionries\n",
    "    # will be saved\n",
    "    #\n",
    "    # returns to_save_img, a list of the dictionaries obtained from each model in model_names\n",
    "    \n",
    "    to_save = []\n",
    "    n=1\n",
    "    for model_path in models_names:\n",
    "\n",
    "        print(f'model number: {n} out of {len(models_names)} \\n {model_path}')\n",
    "        print('load_model...')\n",
    "        model = load_VGG_model(folder + model_path, device)\n",
    "        real_labels, pred_labels, logits = obtain_labels(model, dataset, device)\n",
    "\n",
    "        logits = torch.FloatTensor(logits)\n",
    "        real_labels = torch.FloatTensor(real_labels).to(torch.int64)\n",
    "\n",
    "        print('obtain performance...')\n",
    "        AUROC, AUPRC, ACC = classification_performance(logits, real_labels)\n",
    "\n",
    "        dics_img = {}\n",
    "        dics_img['model'] = model_path[:-3]\n",
    "        dics_img['AUROC'] = AUROC\n",
    "        dics_img['ACC'] = ACC\n",
    "        dics_img['AUPRC'] = AUPRC\n",
    "        dics_img['real_labels'] = real_labels\n",
    "        dics_img['pred_labels'] = pred_labels\n",
    "        dics_img['logits'] = logits\n",
    "        dics_img['block'] = model_path[15]\n",
    "        #inverts the name, finds the index with '_', selects the chars from that (in the not inverted name: len(name)-index) untill the end \n",
    "        dics_img['seed'] = model_path[len(model_path)-model_path[::-1].find('_'):-3]\n",
    "        \n",
    "        to_save.append(dics_img)\n",
    "        \n",
    "        n+=1\n",
    "        print()\n",
    "            \n",
    "    print()\n",
    "    print('===================================')\n",
    "    print('save performances...')\n",
    "    \n",
    "    to_save_dump = to_save\n",
    "    with open(output_file_path+'to_save.pkl', 'wb') as fp:\n",
    "        pickle.dump(to_save_dump, fp)\n",
    "       \n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "efbf1857",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number: 1 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_18464.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 2 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.004_58461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 3 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_646976.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 4 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 5 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.008_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 6 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 7 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.008_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 8 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.008_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 9 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 10 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.008_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 11 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.004_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 12 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.008_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 13 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 14 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 15 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.008_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 16 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.004_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 17 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 18 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.008_58461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 19 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 20 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 21 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.008_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 22 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.008_18464.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 23 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 24 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.004_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 25 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.004_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "\n",
      "===================================\n",
      "save performances...\n"
     ]
    }
   ],
   "source": [
    "folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/best_acc/done/'\n",
    "models_names = os.listdir(folder)\n",
    "models_names = [i for i in models_names if (i[0]=='n' and i.find('MRI')==-1)]\n",
    "\n",
    "device = 'cuda:6'\n",
    "\n",
    "out_folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/best_acc/saves/'\n",
    "\n",
    "best_models_dict_list = save_class_metrics(folder, models_names, dataset, out_folder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2772f9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_finetuning_1conv_0.5_img_2500_0.02_18464 || tensor(0.8280) || tensor(0.7429) || tensor(0.8293)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_23548 || tensor(0.8330) || tensor(0.7457) || tensor(0.8342)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_55168461 || tensor(0.8288) || tensor(0.7444) || tensor(0.8310)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_646976 || tensor(0.8354) || tensor(0.7423) || tensor(0.8373)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_98794515 || tensor(0.8100) || tensor(0.7249) || tensor(0.8124)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_116560000 || tensor(0.9934) || tensor(0.9586) || tensor(0.9936)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_18464876 || tensor(0.9944) || tensor(0.9625) || tensor(0.9945)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_23548 || tensor(0.9931) || tensor(0.9545) || tensor(0.9933)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_32323548 || tensor(0.9939) || tensor(0.9520) || tensor(0.9941)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_55168461 || tensor(0.9927) || tensor(0.9576) || tensor(0.9929)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_116560000 || tensor(0.9996) || tensor(0.9910) || tensor(0.9996)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_18464 || tensor(0.9976) || tensor(0.9810) || tensor(0.9975)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_18464876 || tensor(0.9994) || tensor(0.9886) || tensor(0.9994)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_55168461 || tensor(0.9947) || tensor(0.9650) || tensor(0.9948)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_98794515 || tensor(0.9994) || tensor(0.9890) || tensor(0.9994)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_116560000 || tensor(0.9995) || tensor(0.9891) || tensor(0.9995)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_18464876 || tensor(0.9991) || tensor(0.9847) || tensor(0.9991)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_32323548 || tensor(0.9994) || tensor(0.9883) || tensor(0.9994)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_58461 || tensor(0.9998) || tensor(0.9932) || tensor(0.9998)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_98794515 || tensor(0.9998) || tensor(0.9944) || tensor(0.9998)\n",
      "new_finetuning_all_0.5_img_2500_0.004_116560000 || tensor(0.9998) || tensor(0.9941) || tensor(0.9999)\n",
      "new_finetuning_all_0.5_img_2500_0.004_23548 || tensor(0.9998) || tensor(0.9931) || tensor(0.9998)\n",
      "new_finetuning_all_0.5_img_2500_0.004_32323548 || tensor(0.9997) || tensor(0.9923) || tensor(0.9997)\n",
      "new_finetuning_all_0.5_img_2500_0.004_55168461 || tensor(0.9995) || tensor(0.9893) || tensor(0.9995)\n",
      "new_finetuning_all_0.5_img_2500_0.004_58461 || tensor(0.9987) || tensor(0.9848) || tensor(0.9987)\n"
     ]
    }
   ],
   "source": [
    "for name in sorted([n['model'] for n in best_models_dict_list]):\n",
    "    for i in best_models_dict_list:\n",
    "        if i['model']==name:\n",
    "            print(i['model'], '||', i['AUROC'],'||', i['ACC'], '||', i['AUPRC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42c59432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.8270662426948547 || ACC: 0.7400565147399902 || AUPRC: 0.8288397789001465\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9935058355331421 || ACC: 0.9570363759994507 || AUPRC: 0.9936791658401489\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9981449842453003 || ACC: 0.9829277992248535 || AUPRC: 0.9981642961502075\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9995126724243164 || ACC: 0.9899336695671082 || AUPRC: 0.9995200037956238\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9995056986808777 || ACC: 0.9907243847846985 || AUPRC: 0.9995131492614746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in sorted(set([n['block'] for n in best_models_dict_list])):\n",
    "    AUROC_mean = 0\n",
    "    ACC_mean = 0\n",
    "    AUPRC_mean = 0 \n",
    "    \n",
    "    for i in best_models_dict_list:\n",
    "        if i['block']==b:\n",
    "            AUROC_mean += i['AUROC']\n",
    "            ACC_mean += i['ACC']\n",
    "            AUPRC_mean += i['AUPRC']\n",
    "    \n",
    "    print(f'Average classification performance based on the 5 models')\n",
    "    print(f'AUROC: {AUROC_mean/5} || ACC: {ACC_mean/5} || AUPRC: {AUPRC_mean/5}' )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d61dcb",
   "metadata": {},
   "source": [
    "### Same performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e3bb356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number: 1 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.0006_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 2 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.0018_2147893.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 3 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.0008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 4 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.0006_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 5 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.0018_13246.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 6 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.0006_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 7 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.0006_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 8 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.0018_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 9 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.0006_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 10 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.0006_58461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 11 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.0018_5484646.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 12 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.0008_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 13 out of 25 \n",
      " new_finetuning_all_0.5_img_2500_0.0006_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 14 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_646976.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 15 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 16 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 17 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.0006_646976.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 18 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 19 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.0008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 20 out of 25 \n",
      " new_finetuning_2conv_0.5_img_2500_0.0018_6497.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 21 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.0006_4687687.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 22 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.0008_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 23 out of 25 \n",
      " new_finetuning_3conv_0.5_img_2500_0.0008_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 24 out of 25 \n",
      " new_finetuning_1conv_0.5_img_2500_0.02_18464.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "model number: 25 out of 25 \n",
      " new_finetuning_4conv_0.5_img_2500_0.0006_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "\n",
      "===================================\n",
      "save performances...\n"
     ]
    }
   ],
   "source": [
    "folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/done_same_acc/'\n",
    "models_names = os.listdir(folder)\n",
    "models_names = [i for i in models_names if (i[0]=='n' and i.find('MRI')==-1)]\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "out_folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/done_same_acc/saves/'\n",
    "\n",
    "to_save_same_img = best_models_dict_list = save_class_metrics(folder, models_names, dataset, out_folder, device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "08d2fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_finetuning_1conv_0.5_img_2500_0.02_18464 || tensor(0.8283) || tensor(0.7440) || tensor(0.8290)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_23548 || tensor(0.8316) || tensor(0.7456) || tensor(0.8323)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_55168461 || tensor(0.8277) || tensor(0.7434) || tensor(0.8296)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_646976 || tensor(0.8360) || tensor(0.7425) || tensor(0.8376)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_98794515 || tensor(0.8103) || tensor(0.7230) || tensor(0.8132)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_116560000 || tensor(0.9576) || tensor(0.8827) || tensor(0.9590)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_13246 || tensor(0.9585) || tensor(0.8834) || tensor(0.9601)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_2147893 || tensor(0.9487) || tensor(0.8677) || tensor(0.9505)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_5484646 || tensor(0.9558) || tensor(0.8808) || tensor(0.9574)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_6497 || tensor(0.9542) || tensor(0.8722) || tensor(0.9555)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_116560000 || tensor(0.9485) || tensor(0.8696) || tensor(0.9497)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_18464876 || tensor(0.9615) || tensor(0.8889) || tensor(0.9626)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_23548 || tensor(0.9554) || tensor(0.8830) || tensor(0.9565)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_32323548 || tensor(0.9601) || tensor(0.8881) || tensor(0.9612)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_55168461 || tensor(0.9595) || tensor(0.8919) || tensor(0.9604)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_23548 || tensor(0.9512) || tensor(0.8796) || tensor(0.9522)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_32323548 || tensor(0.9601) || tensor(0.8891) || tensor(0.9610)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_4687687 || tensor(0.9468) || tensor(0.8767) || tensor(0.9476)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_58461 || tensor(0.9534) || tensor(0.8811) || tensor(0.9542)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_646976 || tensor(0.9514) || tensor(0.8728) || tensor(0.9524)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_116560000 || tensor(0.9480) || tensor(0.8747) || tensor(0.9484)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_18464876 || tensor(0.9578) || tensor(0.8895) || tensor(0.9589)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_23548 || tensor(0.9536) || tensor(0.8853) || tensor(0.9545)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_32323548 || tensor(0.9622) || tensor(0.8936) || tensor(0.9631)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_55168461 || tensor(0.9594) || tensor(0.8898) || tensor(0.9601)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name in sorted([n['model'] for n in to_save_same_img]):\n",
    "    for i in to_save_img_same:\n",
    "        if i['model']==name:\n",
    "            print(i['model'], '||', i['AUROC'],'||', i['ACC'], '||', i['AUPRC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fd0a7cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.8267999887466431 || ACC: 0.7396830916404724 || AUPRC: 0.8283335566520691\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9549658894538879 || ACC: 0.8773702383041382 || AUPRC: 0.9565150141716003\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9570051431655884 || ACC: 0.8842871785163879 || AUPRC: 0.9580898284912109\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9525769352912903 || ACC: 0.8798591494560242 || AUPRC: 0.9534769058227539\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9561964273452759 || ACC: 0.8865715861320496 || AUPRC: 0.9570118188858032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in sorted(set([n['block'] for n in to_save_same_img])):\n",
    "    AUROC_mean = 0\n",
    "    ACC_mean = 0\n",
    "    AUPRC_mean = 0 \n",
    "    \n",
    "    for i in to_save_img_same:\n",
    "        if i['block']==b:\n",
    "            AUROC_mean += i['AUROC']\n",
    "            ACC_mean += i['ACC']\n",
    "            AUPRC_mean += i['AUPRC']\n",
    "    \n",
    "    print(f'Average classification performance based on the 5 models')\n",
    "    print(f'AUROC: {AUROC_mean/5} || ACC: {ACC_mean/5} || AUPRC: {AUPRC_mean/5}' )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5632d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
