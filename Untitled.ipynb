{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29790a5",
   "metadata": {},
   "source": [
    "# Imagenet results\n",
    "+ results for correctly classified\n",
    "    + quantitative\n",
    "    + qualitative\n",
    "+ results for incorrectly classified\n",
    "     + quantitative\n",
    "     + qualitative\n",
    "+ two additional performance metric\n",
    "     + AUROC\n",
    "     + AUPRC\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7e97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "from torchmetrics.classification import BinaryAveragePrecision\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "from torchmetrics.classification import MulticlassAUROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85e0371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "torchmetrics.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55feef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import transform\n",
    "from skimage.transform import resize\n",
    "import skimage.exposure as skie\n",
    "\n",
    "import ot\n",
    "\n",
    "import torch\n",
    "from torch import manual_seed\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import Image \n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.set_num_threads(3)\n",
    "\n",
    "def compactness(blob_labels):\n",
    "    import math\n",
    "    compactness=[]\n",
    "    region=regionprops(blob_labels)\n",
    "    for rp in region:\n",
    "        area=rp.area\n",
    "        #perimeter=rp.perimeter\n",
    "        perimeter=rp.perimeter_crofton\n",
    "        c=(4*math.pi*area)/(perimeter**2)\n",
    "        compactness.append(c)\n",
    "    return compactness\n",
    "\n",
    "def get_blobs(image,lesion_size):\n",
    "    \"\"\"\n",
    "    gets the noise image after pre processing and returns blobs with the size equal to lesion size\n",
    "    image is the noise image \n",
    "    lesion_size can be an int\n",
    "    \"\"\"\n",
    "    \n",
    "    labeled, nr_objects = ndimage.label(image)\n",
    "    sizes = ndimage.sum_labels(image,labeled,range(nr_objects+1))    \n",
    "    mask_size=sizes!=lesion_size\n",
    "    small_blobs=labeled.copy()\n",
    "    remove_pixel = mask_size[small_blobs]\n",
    "    small_blobs[remove_pixel] = 0 \n",
    "    \n",
    "    return small_blobs\n",
    "\n",
    "\n",
    "def list_lesions(image,lesion_size):\n",
    "    round_lesion_c=0.8 #lesions with compactness above this value are considered round\n",
    "    not_round_lesion_c=0.4 #lesions with compactness bellow this value are considered not round\n",
    "    \n",
    "    border=2\n",
    "    blur=0.75\n",
    "    \n",
    "    less_size=lesion_size*0.05*blur #the lesion is smaller after the blur\n",
    "    \n",
    "    small_blobs=get_blobs(image,lesion_size)\n",
    "    rb=regionprops(small_blobs)\n",
    "    round_lesions=[]\n",
    "    not_round_lesions=[]\n",
    "    c_list=compactness(small_blobs)\n",
    "    \n",
    "    for blob in range(len(c_list)):\n",
    "        \n",
    "        if c_list[blob]>round_lesion_c:#round lesions\n",
    "\n",
    "            blob_img=rb[blob].image.astype(float)\n",
    "            #the image is padded because when it is smoothed it increases a bit\n",
    "            pad_img=np.pad(array=blob_img, pad_width=border, mode='constant', constant_values=0)\n",
    "            blur_image=ndimage.gaussian_filter(pad_img, blur)\n",
    "            blur_image[blur_image<0.2]=0\n",
    "            energy=round(np.sum(blur_image.astype(float)),2)\n",
    "            if energy<=lesion_size-less_size+1 and energy>=lesion_size-less_size-1:\n",
    "                round_lesions.append([blur_image,round(c_list[blob],3)])\n",
    "        \n",
    "        \n",
    "        elif c_list[blob]<not_round_lesion_c:# not round lesions\n",
    "            blob_img=rb[blob].image.astype(float)\n",
    "            #the image is padded because when it is smoothed it increases a bit\n",
    "            pad_img=np.pad(array=blob_img, pad_width=border, mode='constant', constant_values=0)\n",
    "            blur_image=ndimage.gaussian_filter(pad_img, blur)\n",
    "            blur_image[blur_image<0.15]=0\n",
    "            energy=round(np.sum(blur_image.astype(float)),2)\n",
    "            if energy<=lesion_size-less_size+1 and energy>=lesion_size-less_size-1:\n",
    "                not_round_lesions.append([blur_image,round(c_list[blob],3)])\n",
    "            \n",
    "    return round_lesions, not_round_lesions\n",
    "\n",
    "def create_lesions(lesion_number,lesion_size,factor=1):\n",
    "    #factor is the value for which we multiply the sides of the lesion\n",
    "    round_lesions=[]\n",
    "    not_round_lesions=[]\n",
    "    s=0\n",
    "    size_noise=256\n",
    "    blur_radius=2\n",
    "\n",
    "    while len(not_round_lesions)<=lesion_number or len(round_lesions)<=lesion_number:\n",
    "        #create noise\n",
    "        np.random.seed(s)\n",
    "        noise_img=np.random.rand(size_noise,size_noise)\n",
    "\n",
    "        #smooth noise\n",
    "        imgf=ndimage.gaussian_filter(noise_img, blur_radius)\n",
    "\n",
    "        #create binary image\n",
    "        thr=threshold_otsu(imgf)\n",
    "        imgf_bin=imgf>thr\n",
    "        \n",
    "        #morphologic changes\n",
    "        erosion_image=ndimage.binary_erosion(imgf_bin)\n",
    "        open_er_img=ndimage.binary_opening(erosion_image)\n",
    "        erosion_image2=ndimage.binary_erosion(open_er_img)\n",
    "\n",
    "        #images\n",
    "        \n",
    "\n",
    "        #as a result from one noise image we create several images that can be used to create the lesions\n",
    "        round_lesions_open, not_round_lesions_open=list_lesions(open_er_img,lesion_size)\n",
    "        round_lesions_er, not_round_lesions_er=list_lesions(erosion_image2,lesion_size)\n",
    "        \n",
    "        round_lesions=round_lesions+round_lesions_open+round_lesions_er\n",
    "        not_round_lesions=not_round_lesions+not_round_lesions_open+not_round_lesions_er\n",
    "        \n",
    "        \n",
    "        #print('len lists:',len(round_lesions),len(not_round_lesions))\n",
    "        '''        \n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.subplot(2,4,1)\n",
    "        plt.imshow(noise_img)\n",
    "        plt.title('noise')\n",
    "        plt.subplot(2,4,2)\n",
    "        plt.imshow(imgf)\n",
    "        plt.title('blur')\n",
    "        plt.subplot(2,4,3)\n",
    "        plt.imshow(imgf_bin)\n",
    "        plt.title('binary img')\n",
    "        plt.subplot(2,4,4)\n",
    "        plt.imshow(erosion_image)\n",
    "        plt.title('erosion')\n",
    "        plt.subplot(2,4,5)\n",
    "        plt.imshow(open_er_img)\n",
    "        plt.title('open')\n",
    "        plt.subplot(2,4,6)\n",
    "        plt.imshow(erosion_image2)\n",
    "        plt.title('erosion2')\n",
    "        plt.subplot(2,4,7)\n",
    "        plt.imshow(dilated_image)\n",
    "        plt.title('dilation')\n",
    "        break\n",
    "        '''\n",
    "        \n",
    "        s+=1\n",
    "    print(f'round lesions: {len(round_lesions)}/{lesion_number} === not round lesions {len(not_round_lesions)}/{lesion_number}')\n",
    "    print(f'number of seeds used: {s-1}')\n",
    "    print(f'the lesions have size between {lesion_size-lesion_size*0.05*0.75-1} and {lesion_size-lesion_size*0.05*0.75+1}')\n",
    "    \n",
    "    lesions_r=round_lesions[:lesion_number]\n",
    "    lesions_nr=not_round_lesions[:lesion_number]\n",
    "    round_lesions=[[np.array(resize(round_lesions[i][0],(round_lesions[i][0].shape[0]*factor,round_lesions[i][0].shape[1]*factor))),round_lesions[i][1]] for i in range(len(lesions_r))]\n",
    "    not_round_lesions=[[np.array(resize(not_round_lesions[i][0],(not_round_lesions[i][0].shape[0]*factor,not_round_lesions[i][0].shape[1]*factor))),not_round_lesions[i][1]] for i in range(len(lesions_nr))]\n",
    "    \n",
    "    return round_lesions, not_round_lesions\n",
    "\n",
    "def rescale_values(image,max_val,min_val):\n",
    "    '''\n",
    "    image - numpy array\n",
    "    max_val/min_val - float\n",
    "    '''\n",
    "    return (image-image.min())/(image.max()-image.min())*(max_val-min_val)+min_val\n",
    "\n",
    "def select_coordinates(slice_image, lesions,white_constant,seed):\n",
    "    '''\n",
    "    slice_image is the brain slice to use\n",
    "    lesions is a list of the lesions (with len=number_of_lesions) to use\n",
    "    colour_lesion is either 'black' or 'white'\n",
    "    white_constant is the constant that is multiplied with the lesion mask to create lighter or darker lesions\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    brain_mask=np.array(slice_image)>0\n",
    "    brain_mask=brain_mask.astype(float)\n",
    "    x,y = np.where(brain_mask == 1.)\n",
    "    \n",
    "    lesion_brain=slice_image.copy().astype(float)\n",
    "    lesion_mask=brain_mask.copy() \n",
    "    lesion_added=0\n",
    "    ground_truth=np.zeros(slice_image.shape)\n",
    "    min_value=0.1 #min value for the lesion intensity \n",
    "    max_value=0.9 #max value for the lesion intensity\n",
    "    brain_image=slice_image.copy()\n",
    "    \n",
    "    while lesion_added<len(lesions):\n",
    "        i=np.random.choice(np.arange(len(x)))\n",
    "        coordinate=[x[i],y[i]]\n",
    "        lesion=lesions[lesion_added]\n",
    "        lesion_rescale = rescale_values(lesion,max_value,min_value)\n",
    "        lesion_rescale=rescale_values(lesion,white_constant,min_value)\n",
    "        \n",
    "        #creating the lesion mask and ground truth\n",
    "        if (brain_mask[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]] ==1).all():\n",
    "            #checks if the lesion that will be added is completly in a white space of the lesion mask \n",
    "            #(this means that the new lesion is not overlaping an existing one and is completly in the brain area)\n",
    "            lesion_mask[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]] -= lesion_rescale\n",
    "            lesion_added+=1\n",
    "            ground_truth[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]]+=lesion\n",
    "            brain_mask=lesion_mask\n",
    "\n",
    "    \n",
    "    brain_image=slice_image.copy()\n",
    "    brain_mask=np.array(slice_image)>0\n",
    "    #creating the white lesions\n",
    "       \n",
    "    brain_image[brain_image>0]=1-brain_image[brain_image>0]        \n",
    "    brain_image[lesion_mask!=0]*=lesion_mask[lesion_mask!=0]\n",
    "    brain_image[brain_mask]=1-brain_image[brain_mask]\n",
    "    \n",
    "\n",
    "    \n",
    "    return lesion_mask,brain_image,ground_truth\n",
    "    \n",
    "def add_lesions(slice_image,round_lesions,not_round_lesions,min_lesion,max_lesion,white_constant,seed,max_brain):\n",
    "    #for each slice we chose: random number of lesions, random lesions, random coordinates\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    number_of_lesions=np.random.randint(min_lesion,max_lesion+1)\n",
    "    lesion_type=np.random.randint(0,2)\n",
    "    #lesion_type=0 - round\n",
    "    #lesion_type=1 - not round\n",
    "    \n",
    "    #get lesions from type of lesions (and target)\n",
    "    added=rescale_values(slice_image.copy(),max_brain,0)\n",
    "    if lesion_type==0: #round\n",
    "        target=0\n",
    "        with open('round_lesions.pkl', 'rb') as f:\n",
    "            lesion_list=pickle.load(f)\n",
    "        np.random.shuffle(round_lesions)\n",
    "        \n",
    "    elif lesion_type==1: #not round\n",
    "        target=1\n",
    "        with open('not_round_lesions.pkl', 'rb') as f:\n",
    "            lesion_list=pickle.load(f) \n",
    "        np.random.shuffle(lesion_list)\n",
    "        \n",
    "    lesions=[i[0] for i in lesion_list[:number_of_lesions]]\n",
    "    #add the lesions\n",
    "    \n",
    "    lesion_mask,lesion_brain_white,ground_truth=select_coordinates(added, lesions,white_constant,seed)\n",
    "    \n",
    "    \n",
    "    return lesion_mask,lesion_brain_white,ground_truth,target,number_of_lesions\n",
    "\n",
    "def change_images(image):\n",
    "    image=np.repeat(image[..., np.newaxis], 3, axis=2)\n",
    "    image=resize(image, (224, 224))\n",
    "    image=image.transpose(2,0,1)\n",
    "    return image\n",
    "\n",
    "def create_dataset(slices,round_lesions,not_round_lesions,min_lesion=3,max_lesion=5,white_constant=0.85,seed=0,max_brain=1):\n",
    "\n",
    "    dataset_white=[]\n",
    "    number_lesions=[]\n",
    "    lesion_mask_list=[]\n",
    "    ground_truths=[]\n",
    "    for slice_idx in range(len(slices)):\n",
    "        lesion_mask,lesion_brain_white,ground_truth,target,number_of_lesions=add_lesions(slices[slice_idx],\n",
    "                                                                                         round_lesions,\n",
    "                                                                                         not_round_lesions,\n",
    "                                                                                         min_lesion=min_lesion,\n",
    "                                                                                         max_lesion=max_lesion,\n",
    "                                                                                         white_constant=white_constant,\n",
    "                                                                                        seed=seed,\n",
    "                                                                                        max_brain=max_brain)\n",
    "        dataset_white.append([change_images(lesion_brain_white),target])\n",
    "        number_lesions.append(number_of_lesions)\n",
    "        lesion_mask_list.append(lesion_mask)\n",
    "        ground_truths.append(ground_truth)\n",
    "        seed+=1\n",
    "        \n",
    "        if slice_idx%1500==0:\n",
    "            print(f'slice {slice_idx}/{len(slices)} = {round(100*slice_idx/len(slices),2)}%')\n",
    "        \n",
    "    return dataset_white,number_lesions,lesion_mask_list,ground_truths\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d2ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VGG_model(path,device):\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.classifier=model.classifier[:-1]\n",
    "    last_layers=[nn.Linear(4096,2)]\n",
    "    model.classifier = nn.Sequential(*list(model.classifier)+last_layers) \n",
    "\n",
    "    model.load_state_dict(torch.load(path,map_location=device))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724132f",
   "metadata": {},
   "source": [
    "### Best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be452fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_labels(model, dataset, DEVICE):\n",
    "    BATCH = 32\n",
    "    dataloader = DataLoader(dataset,batch_size=BATCH)\n",
    "\n",
    "    real_labels = []\n",
    "    pred_labels = []\n",
    "    logits_list = []\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "\n",
    "        inputs = inputs.to(DEVICE,dtype=torch.float)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        real_labels += torch.Tensor.cpu(labels).tolist()\n",
    "        \n",
    "        logits_list += torch.Tensor.cpu(logits).tolist()\n",
    "        \n",
    "        pred_labels += torch.Tensor.cpu(torch.max(logits, 1)[1]).tolist()\n",
    "        \n",
    "    model=None\n",
    "    inputs=None\n",
    "    labels=None\n",
    "    logits=None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return real_labels, pred_labels, logits_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "117b1692",
   "metadata": {},
   "outputs": [],
   "source": [
    " model=None\n",
    "inputs=None\n",
    "labels=None\n",
    "logits=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79abf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_performance(logits, target):\n",
    "#     metric = BinaryAUROC(thresholds=None)\n",
    "#     AUROC = metric(logits, target)\n",
    "    \n",
    "#     metric = BinaryAveragePrecision(thresholds=None)\n",
    "#     AUPRC = metric(logits, target) # preds are logits  \n",
    "    \n",
    "#     metric = BinaryAccuracy()\n",
    "#     ACC = metric(logits, target)\n",
    "\n",
    "    metric = MulticlassAveragePrecision(num_classes=2)\n",
    "    AUPRC = metric(logits, target)\n",
    "\n",
    "    metric = MulticlassAccuracy(num_classes=2)\n",
    "    ACC = metric(logits, target)\n",
    "\n",
    "    metric = MulticlassAUROC(num_classes=2)\n",
    "    AUROC = metric(logits, target)\n",
    "\n",
    "    return AUROC, AUPRC, ACC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce87b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_precision_explanation(explanation,gt):\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "febb2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mcuda01                       \u001b[m  Fri Mar 24 18:51:17 2023  \u001b[1m\u001b[30m470.161.03\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  278\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m135M\u001b[m) \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m135M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 83'C\u001b[m, \u001b[1m\u001b[32m 87 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 6725\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m6717M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  723\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m715M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  705\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m697M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 28'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  723\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m715M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 33'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1402\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m697M\u001b[m) \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m697M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 60'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1417\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m1409M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4173e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round lesions: 51/50 === not round lesions 207/50\n",
      "number of seeds used: 886\n",
      "the lesions have size between 66.375 and 68.375\n",
      "took 10.5s\n",
      " ====== holdout ====== \n",
      "slice 0/8539 = 0.0%\n",
      "slice 1500/8539 = 17.57%\n",
      "slice 3000/8539 = 35.13%\n",
      "slice 4500/8539 = 52.7%\n",
      "slice 6000/8539 = 70.27%\n",
      "slice 7500/8539 = 87.83%\n",
      "\n",
      "took 153.08s\n",
      "4277 slices of target 1 out of 8539 slices: 50.09 %\n",
      " number of slices: 8539\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3412019/452030037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' number of slices: {len(dataset)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'total time: {(time.time()-t0) // 60:.0f}m {(time.time()-t0) % 60:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't0' is not defined"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "plt.rc('image',cmap='gray')  \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# creating lesions\n",
    "number_of_lesions=50 #amount of lesions in each lesion list\n",
    "size_of_lesions=70 #size of all the lesions\n",
    "factor=2\n",
    "\n",
    "round_lesions, not_round_lesions=create_lesions(number_of_lesions,size_of_lesions,factor=factor)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(f'took {round(elapsed,2)}s')\n",
    "\n",
    "# loading slices \n",
    "with open('slices_validation.pkl', 'rb') as f:\n",
    "    validation_slices,target_valid_gender,target_valid_age = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# adding lesions to slices\n",
    "lesion_max_intensity=0.5\n",
    "max_brain_intensity=0.7\n",
    "\n",
    "start = time.time()\n",
    "print(' ====== holdout ====== ')\n",
    "\n",
    "dataset,_,_,ground_truths=create_dataset(validation_slices,\n",
    "                                          round_lesions,\n",
    "                                          not_round_lesions,\n",
    "                                          min_lesion=3,\n",
    "                                          max_lesion=5,\n",
    "                                          white_constant=lesion_max_intensity,\n",
    "                                          seed=50000,\n",
    "                                          max_brain=max_brain_intensity)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print()\n",
    "print(f'took {round(elapsed,2)}s')\n",
    "target_w=[i[1] for i in dataset]\n",
    "print(f'{len([i for i in target_w if i==1])} slices of target 1 out of {len(target_w)} slices: {round(100*len([i for i in target_w if i==1])/len(target_w),2)} %')\n",
    "print(f' number of slices: {len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e8d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mcuda01                       \u001b[m  Fri Mar 24 17:50:24 2023  \u001b[1m\u001b[30m470.161.03\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  278\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m135M\u001b[m) \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m135M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 69'C\u001b[m, \u001b[1m\u001b[32m 83 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3313\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m3305M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  723\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m715M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 48'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  705\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m697M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 28'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 2525\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m2517M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 64'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m11134\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m5109M\u001b[m) \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m6017M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe1b6de7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_finetuning_1conv_0.5_img_2500_0.02_18464.pt', 'new_finetuning_all_0.5_img_2500_0.004_58461.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_646976.pt', 'new_finetuning_2conv_0.5_img_2500_0.008_18464876.pt', 'new_finetuning_4conv_0.5_img_2500_0.008_32323548.pt', 'new_finetuning_4conv_0.5_img_2500_0.008_116560000.pt', 'new_finetuning_4conv_0.5_img_2500_0.008_98794515.pt', 'new_finetuning_3conv_0.5_img_2500_0.008_98794515.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_23548.pt', 'new_finetuning_3conv_0.5_img_2500_0.008_55168461.pt', 'new_finetuning_all_0.5_img_2500_0.004_23548.pt', 'new_finetuning_2conv_0.5_img_2500_0.008_32323548.pt', 'new_finetuning_2conv_0.5_img_2500_0.008_116560000.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_55168461.pt', 'new_finetuning_2conv_0.5_img_2500_0.008_23548.pt', 'new_finetuning_all_0.5_img_2500_0.004_55168461.pt', 'new_finetuning_4conv_0.5_img_2500_0.008_18464876.pt', 'new_finetuning_4conv_0.5_img_2500_0.008_58461.pt', 'new_finetuning_3conv_0.5_img_2500_0.008_18464876.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_98794515.pt', 'new_finetuning_2conv_0.5_img_2500_0.008_55168461.pt', 'new_finetuning_3conv_0.5_img_2500_0.008_18464.pt', 'new_finetuning_3conv_0.5_img_2500_0.008_116560000.pt', 'new_finetuning_all_0.5_img_2500_0.004_32323548.pt', 'new_finetuning_all_0.5_img_2500_0.004_116560000.pt']\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_18464.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.004_58461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_646976.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.004_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.004_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_58461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_18464.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.004_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.004_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n"
     ]
    }
   ],
   "source": [
    "folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/best_acc/done/'\n",
    "models_names = os.listdir(folder)\n",
    "models_names = [i for i in models_names if i[0]!='.']\n",
    "models_names = [i for i in models_names if (i[0]!='.' and i.find('MRI')==-1)]\n",
    "\n",
    "device = 'cuda:7'\n",
    "\n",
    "to_save_img = []\n",
    "to_save_mri = []\n",
    "\n",
    "for model_path in models_names:\n",
    "    \n",
    "    print(model_path)\n",
    "    print('load_model...')\n",
    "    model = load_VGG_model(folder + model_path, device)\n",
    "    real_labels, pred_labels, logits = obtain_labels(model, dataset, device)\n",
    "    \n",
    "    logits = torch.FloatTensor(logits)\n",
    "    real_labels = torch.FloatTensor(real_labels).to(torch.int64)\n",
    "    \n",
    "    print('obtain performance...')\n",
    "    AUROC, AUPRC, ACC = classification_performance(logits, real_labels)\n",
    "    \n",
    "    if model_path.find('img')!=-1:\n",
    "        dics_img = {}\n",
    "        dics_img['model'] = model_path[:-3]\n",
    "        dics_img['AUROC'] = AUROC\n",
    "        dics_img['ACC'] = ACC\n",
    "        dics_img['AUPRC'] = AUPRC\n",
    "        dics_img['real_labels'] = real_labels\n",
    "        dics_img['pred_labels'] = pred_labels\n",
    "        dics_img['logits'] = logits\n",
    "        dics_img['block'] = model_path[15]\n",
    "        #inverts the name, finds the index with '_', selects the chars from that (in the not inverted name: len(name)-index) untill the end \n",
    "        dics_img['seed'] = model_path[len(model_path)-model_path[::-1].find('_'):-3]\n",
    "\n",
    "        to_save_img.append(dics_img)\n",
    "        \n",
    "    elif model_path.find('MRI')!=-1:\n",
    "        dics_mri = {}\n",
    "        dics_mri['model'] = model_path[:-3]\n",
    "        dics_mri['AUROC'] = AUROC\n",
    "        dics_mri['ACC'] = ACC\n",
    "        dics_mri['AUPRC'] = AUPRC\n",
    "        dics_mri['real_labels'] = real_labels\n",
    "        dics_mri['pred_labels'] = pred_labels\n",
    "        dics_mri['logits'] = logits\n",
    "        dics_mri['block'] = model_path[15]\n",
    "        #inverts the name, finds the index with '_', selects the chars from that (in the not inverted name: len(name)-index) untill the end \n",
    "        dics_mri['seed'] = model_path[len(model_path)-model_path[::-1].find('_'):-3]\n",
    "        \n",
    "        to_save_mri.append(dics_mri)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9461d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "save performances...\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "print()\n",
    "print('===================================')\n",
    "print('save performances...')\n",
    "out_folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/best_acc/saves/'\n",
    "\n",
    "with open(out_folder+'to_save_img.pkl', 'wb') as fp:\n",
    "    pickle.dump(to_save_img, fp)\n",
    "    \n",
    "# with open(out_folder+'to_save_mri.pkl', 'wb') as fp:\n",
    "#     pickle.dump(to_save_mri, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac1cf48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolList2BinString(lst):\n",
    "    # lst is a binary list that corresponds to the comparison between the predicted labels and the real labels\n",
    "    # returns a binary string version of the list\n",
    "    \n",
    "    return '0b' + ''.join(['1' if x else '0' for x in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7164f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bools = []\n",
    "to_save_img\n",
    "to_save_mri\n",
    "\n",
    "for i in to_save_img:\n",
    "    compare_list = np.array(i['real_labels'])==np.array(i['pred_labels'])\n",
    "    bins = int(boolList2BinString(compare_list),2)\n",
    "    bools.append(bins)\n",
    "    \n",
    "for i in to_save_mri:\n",
    "    compare_list = np.array(i['real_labels'])==np.array(i['pred_labels'])\n",
    "    bins = int(boolList2BinString(compare_list),2)\n",
    "    bools.append(bins)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3155a99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8539\n",
      "4540\n"
     ]
    }
   ],
   "source": [
    "# obtain intersection of correctly classified\n",
    "\n",
    "#obtain the intersection of the correctly classified values using bitwize and between the bins and a list of ones \n",
    "value = '1'\n",
    "l=[str(value) for _ in range(len(dataset))]\n",
    "res=int('0b'+''.join(l),2)\n",
    "for i in bools:\n",
    "    res = res & i #bitwise and\n",
    "\n",
    "# calculate the number of correct \n",
    "print(len('{0:08319b}'.format(res)))\n",
    "r='{0:08319b}'.format(res)\n",
    "n=0\n",
    "for i in r:\n",
    "    if i=='1':\n",
    "        n+=1\n",
    "print(n)\n",
    "\n",
    "idx=[i for i,x in enumerate(r) if x=='1']\n",
    "idx\n",
    "correct=[dataset[i] for i in idx]\n",
    "correct_gt=[resize(ground_truths[i],(224,224)) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd275fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_correctly_classified_img = idx\n",
    "with open(out_folder+'best_correctly_classified_img.pkl', 'wb') as fp:\n",
    "    pickle.dump(best_correctly_classified_img, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f11aa374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8539\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# obtain intersection of incorrectly classified\n",
    "\n",
    "#obtain the intersection of the correctly classified values using bitwize or between the bins and a list of zeros \n",
    "value = '0'\n",
    "l=[str(value) for _ in range(len(dataset))]\n",
    "res=int('0b'+''.join(l),2)\n",
    "for i in bools:\n",
    "    res = res | i #bitwise or\n",
    "\n",
    "# calculate the number of correct \n",
    "print(len('{0:08319b}'.format(res)))\n",
    "r='{0:08319b}'.format(res)\n",
    "n=0\n",
    "for i in r:\n",
    "    if i=='0':\n",
    "        n+=1\n",
    "print(n)\n",
    "\n",
    "idx=[i for i,x in enumerate(r) if x=='0']\n",
    "idx\n",
    "incorrect=[dataset[i] for i in idx]\n",
    "incorrect_gt=[resize(ground_truths[i],(224,224)) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34b01929",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_incorrectly_classified_img = idx\n",
    "with open(out_folder+'best_incorrectly_classified_img.pkl', 'wb') as fp:\n",
    "    pickle.dump(best_incorrectly_classified_img, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2772f9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_finetuning_1conv_0.5_img_2500_0.02_18464 || tensor(0.8270) || tensor(0.7409) || tensor(0.8280)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_23548 || tensor(0.8325) || tensor(0.7473) || tensor(0.8330)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_55168461 || tensor(0.8275) || tensor(0.7412) || tensor(0.8290)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_646976 || tensor(0.8357) || tensor(0.7443) || tensor(0.8360)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_98794515 || tensor(0.8093) || tensor(0.7232) || tensor(0.8118)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_116560000 || tensor(0.9934) || tensor(0.9587) || tensor(0.9936)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_18464876 || tensor(0.9945) || tensor(0.9602) || tensor(0.9946)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_23548 || tensor(0.9930) || tensor(0.9532) || tensor(0.9932)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_32323548 || tensor(0.9941) || tensor(0.9537) || tensor(0.9942)\n",
      "new_finetuning_2conv_0.5_img_2500_0.008_55168461 || tensor(0.9925) || tensor(0.9545) || tensor(0.9927)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_116560000 || tensor(0.9995) || tensor(0.9907) || tensor(0.9996)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_18464 || tensor(0.9978) || tensor(0.9802) || tensor(0.9979)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_18464876 || tensor(0.9994) || tensor(0.9883) || tensor(0.9994)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_55168461 || tensor(0.9947) || tensor(0.9644) || tensor(0.9948)\n",
      "new_finetuning_3conv_0.5_img_2500_0.008_98794515 || tensor(0.9994) || tensor(0.9892) || tensor(0.9994)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_116560000 || tensor(0.9995) || tensor(0.9892) || tensor(0.9995)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_18464876 || tensor(0.9991) || tensor(0.9849) || tensor(0.9991)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_32323548 || tensor(0.9994) || tensor(0.9877) || tensor(0.9994)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_58461 || tensor(0.9998) || tensor(0.9930) || tensor(0.9998)\n",
      "new_finetuning_4conv_0.5_img_2500_0.008_98794515 || tensor(0.9997) || tensor(0.9947) || tensor(0.9997)\n",
      "new_finetuning_all_0.5_img_2500_0.004_116560000 || tensor(0.9998) || tensor(0.9944) || tensor(0.9998)\n",
      "new_finetuning_all_0.5_img_2500_0.004_23548 || tensor(0.9997) || tensor(0.9929) || tensor(0.9997)\n",
      "new_finetuning_all_0.5_img_2500_0.004_32323548 || tensor(0.9997) || tensor(0.9922) || tensor(0.9997)\n",
      "new_finetuning_all_0.5_img_2500_0.004_55168461 || tensor(0.9994) || tensor(0.9885) || tensor(0.9994)\n",
      "new_finetuning_all_0.5_img_2500_0.004_58461 || tensor(0.9989) || tensor(0.9856) || tensor(0.9989)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name in sorted([n['model'] for n in to_save_img]):\n",
    "    for i in to_save_img:\n",
    "        if i['model']==name:\n",
    "            print(i['model'], '||', i['AUROC'],'||', i['ACC'], '||', i['AUPRC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42c59432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.8263908624649048 || ACC: 0.7393766641616821 || AUPRC: 0.8275675773620605\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.993503212928772 || ACC: 0.9560527801513672 || AUPRC: 0.9936662912368774\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9981697201728821 || ACC: 0.9825762510299683 || AUPRC: 0.9982060194015503\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9995044469833374 || ACC: 0.9899104237556458 || AUPRC: 0.9995114207267761\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9995091557502747 || ACC: 0.9907007217407227 || AUPRC: 0.9995158314704895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in sorted(set([n['block'] for n in to_save_img])):\n",
    "    AUROC_mean = 0\n",
    "    ACC_mean = 0\n",
    "    AUPRC_mean = 0 \n",
    "    \n",
    "    for i in to_save_img:\n",
    "        if i['block']==b:\n",
    "            AUROC_mean += i['AUROC']\n",
    "            ACC_mean += i['ACC']\n",
    "            AUPRC_mean += i['AUPRC']\n",
    "    \n",
    "    print(f'Average classification performance based on the 5 models')\n",
    "    print(f'AUROC: {AUROC_mean/5} || ACC: {ACC_mean/5} || AUPRC: {AUPRC_mean/5}' )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa9872ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d61dcb",
   "metadata": {},
   "source": [
    "### Same performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e3bb356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_finetuning_all_0.5_img_2500_0.0006_32323548.pt', 'new_finetuning_2conv_0.5_img_2500_0.0018_2147893.pt', 'new_finetuning_3conv_0.5_img_2500_0.0008_18464876.pt', 'new_finetuning_4conv_0.5_img_2500_0.0006_23548.pt', 'new_finetuning_2conv_0.5_img_2500_0.0018_13246.pt', 'new_finetuning_all_0.5_img_2500_0.0006_55168461.pt', 'new_finetuning_all_0.5_img_2500_0.0006_23548.pt', 'new_finetuning_2conv_0.5_img_2500_0.0018_116560000.pt', 'new_finetuning_all_0.5_img_2500_0.0006_116560000.pt', 'new_finetuning_4conv_0.5_img_2500_0.0006_58461.pt', 'new_finetuning_2conv_0.5_img_2500_0.0018_5484646.pt', 'new_finetuning_3conv_0.5_img_2500_0.0008_32323548.pt', 'new_finetuning_all_0.5_img_2500_0.0006_18464876.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_646976.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_55168461.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_23548.pt', 'new_finetuning_4conv_0.5_img_2500_0.0006_646976.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_98794515.pt', 'new_finetuning_3conv_0.5_img_2500_0.0008_116560000.pt', 'new_finetuning_2conv_0.5_img_2500_0.0018_6497.pt', 'new_finetuning_4conv_0.5_img_2500_0.0006_4687687.pt', 'new_finetuning_3conv_0.5_img_2500_0.0008_23548.pt', 'new_finetuning_3conv_0.5_img_2500_0.0008_55168461.pt', 'new_finetuning_1conv_0.5_img_2500_0.02_18464.pt', 'new_finetuning_4conv_0.5_img_2500_0.0006_32323548.pt']\n",
      "new_finetuning_all_0.5_img_2500_0.0006_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_2147893.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_13246.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.0006_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.0006_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.0006_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_58461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_5484646.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_all_0.5_img_2500_0.0006_18464876.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_646976.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_646976.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_98794515.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_116560000.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_6497.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_4687687.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_23548.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_55168461.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_18464.pt\n",
      "load_model...\n",
      "obtain performance...\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_32323548.pt\n",
      "load_model...\n",
      "obtain performance...\n"
     ]
    }
   ],
   "source": [
    "folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/done_same_acc/'\n",
    "models_names = os.listdir(folder)\n",
    "models_names = [i for i in models_names if (i[0]!='.' and i.find('MRI') == -1)]\n",
    "\n",
    "device = 'cuda:7'\n",
    "\n",
    "to_save_img_same = []\n",
    "to_save_mri_same = []\n",
    "\n",
    "\n",
    "for model_path in models_names:\n",
    "    print(model_path)\n",
    "    print('load_model...')\n",
    "    model = load_VGG_model(folder + model_path, device)\n",
    "    real_labels, pred_labels, logits = obtain_labels(model, dataset, device)\n",
    "    \n",
    "    logits = torch.FloatTensor(logits)\n",
    "    real_labels = torch.FloatTensor(real_labels).to(torch.int64)\n",
    "    \n",
    "    print('obtain performance...')\n",
    "    AUROC, AUPRC, ACC = classification_performance(logits, real_labels)\n",
    "    \n",
    "    if model_path.find('img')!=-1:\n",
    "        dics_img = {}\n",
    "        dics_img['model'] = model_path[:-3]\n",
    "        dics_img['AUROC'] = AUROC\n",
    "        dics_img['ACC'] = ACC\n",
    "        dics_img['AUPRC'] = AUPRC\n",
    "        dics_img['real_labels'] = real_labels\n",
    "        dics_img['pred_labels'] = pred_labels\n",
    "        dics_img['logits'] = logits\n",
    "        dics_img['block'] = model_path[15]\n",
    "        #inverts the name, finds the index with '_', selects the chars from that (in the not inverted name: len(name)-index) untill the end \n",
    "        dics_img['seed'] = model_path[len(model_path)-model_path[::-1].find('_'):-3]\n",
    "\n",
    "        to_save_img_same.append(dics_img)\n",
    "        \n",
    "    elif model_path.find('MRI')!=-1:\n",
    "        dics_mri = {}\n",
    "        dics_mri['model'] = model_path[:-3]\n",
    "        dics_mri['AUROC'] = AUROC\n",
    "        dics_mri['ACC'] = ACC\n",
    "        dics_mri['AUPRC'] = AUPRC\n",
    "        dics_mri['real_labels'] = real_labels\n",
    "        dics_mri['pred_labels'] = pred_labels\n",
    "        dics_mri['logits'] = logits\n",
    "        dics_mri['block'] = model_path[15]\n",
    "        #inverts the name, finds the index with '_', selects the chars from that (in the not inverted name: len(name)-index) untill the end \n",
    "        dics_mri['seed'] = model_path[len(model_path)-model_path[::-1].find('_'):-3]\n",
    "        \n",
    "        to_save_mri_same.append(dics_mri)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee9d7e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "save performances...\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "print()\n",
    "print('===================================')\n",
    "print('save performances...')\n",
    "out_folder = '/home/martao/MRI_dataset/2ndTry/Models/VGG/2500/best_acc/saves/'\n",
    "\n",
    "with open(out_folder+'to_save_img_same.pkl', 'wb') as fp:\n",
    "    pickle.dump(to_save_img, fp)\n",
    "    \n",
    "# with open(out_folder+'to_save_mri.pkl', 'wb') as fp:\n",
    "#     pickle.dump(to_save_mri, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08cb1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bools = []\n",
    "to_save_img\n",
    "to_save_mri\n",
    "\n",
    "for i in to_save_img_same:\n",
    "    compare_list = np.array(i['real_labels'])==np.array(i['pred_labels'])\n",
    "    bins = int(boolList2BinString(compare_list),2)\n",
    "    bools.append(bins)\n",
    "    \n",
    "for i in to_save_mri:\n",
    "    compare_list = np.array(i['real_labels'])==np.array(i['pred_labels'])\n",
    "    bins = int(boolList2BinString(compare_list),2)\n",
    "    bools.append(bins)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34a2bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8539\n",
      "4250\n"
     ]
    }
   ],
   "source": [
    "# obtain intersection of correctly classified\n",
    "\n",
    "#obtain the intersection of the correctly classified values using bitwize and between the bins and a list of ones \n",
    "value = '1'\n",
    "l=[str(value) for _ in range(len(dataset))]\n",
    "res=int('0b'+''.join(l),2)\n",
    "for i in bools:\n",
    "    res = res & i #bitwise and\n",
    "\n",
    "# calculate the number of correct \n",
    "print(len('{0:08319b}'.format(res)))\n",
    "r='{0:08319b}'.format(res)\n",
    "n=0\n",
    "for i in r:\n",
    "    if i=='1':\n",
    "        n+=1\n",
    "print(n)\n",
    "\n",
    "idx=[i for i,x in enumerate(r) if x=='1']\n",
    "idx\n",
    "correct=[dataset[i] for i in idx]\n",
    "correct_gt=[resize(ground_truths[i],(224,224)) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d30bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8539\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "# obtain intersection of incorrectly classified\n",
    "\n",
    "#obtain the intersection of the correctly classified values using bitwize or between the bins and a list of zeros \n",
    "value = '0'\n",
    "l=[str(value) for _ in range(len(dataset))]\n",
    "res=int('0b'+''.join(l),2)\n",
    "for i in bools:\n",
    "    res = res | i #bitwise or\n",
    "\n",
    "# calculate the number of correct \n",
    "print(len('{0:08319b}'.format(res)))\n",
    "r='{0:08319b}'.format(res)\n",
    "n=0\n",
    "for i in r:\n",
    "    if i=='0':\n",
    "        n+=1\n",
    "print(n)\n",
    "\n",
    "idx=[i for i,x in enumerate(r) if x=='0']\n",
    "idx\n",
    "incorrect=[dataset[i] for i in idx]\n",
    "incorrect_gt=[resize(ground_truths[i],(224,224)) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08d2fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_finetuning_1conv_0.5_img_2500_0.02_18464 || tensor(0.8283) || tensor(0.7440) || tensor(0.8290)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_23548 || tensor(0.8316) || tensor(0.7456) || tensor(0.8323)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_55168461 || tensor(0.8277) || tensor(0.7434) || tensor(0.8296)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_646976 || tensor(0.8360) || tensor(0.7425) || tensor(0.8376)\n",
      "new_finetuning_1conv_0.5_img_2500_0.02_98794515 || tensor(0.8103) || tensor(0.7230) || tensor(0.8132)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_116560000 || tensor(0.9576) || tensor(0.8827) || tensor(0.9590)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_13246 || tensor(0.9585) || tensor(0.8834) || tensor(0.9601)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_2147893 || tensor(0.9487) || tensor(0.8677) || tensor(0.9505)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_5484646 || tensor(0.9558) || tensor(0.8808) || tensor(0.9574)\n",
      "new_finetuning_2conv_0.5_img_2500_0.0018_6497 || tensor(0.9542) || tensor(0.8722) || tensor(0.9555)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_116560000 || tensor(0.9485) || tensor(0.8696) || tensor(0.9497)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_18464876 || tensor(0.9615) || tensor(0.8889) || tensor(0.9626)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_23548 || tensor(0.9554) || tensor(0.8830) || tensor(0.9565)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_32323548 || tensor(0.9601) || tensor(0.8881) || tensor(0.9612)\n",
      "new_finetuning_3conv_0.5_img_2500_0.0008_55168461 || tensor(0.9595) || tensor(0.8919) || tensor(0.9604)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_23548 || tensor(0.9512) || tensor(0.8796) || tensor(0.9522)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_32323548 || tensor(0.9601) || tensor(0.8891) || tensor(0.9610)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_4687687 || tensor(0.9468) || tensor(0.8767) || tensor(0.9476)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_58461 || tensor(0.9534) || tensor(0.8811) || tensor(0.9542)\n",
      "new_finetuning_4conv_0.5_img_2500_0.0006_646976 || tensor(0.9514) || tensor(0.8728) || tensor(0.9524)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_116560000 || tensor(0.9480) || tensor(0.8747) || tensor(0.9484)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_18464876 || tensor(0.9578) || tensor(0.8895) || tensor(0.9589)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_23548 || tensor(0.9536) || tensor(0.8853) || tensor(0.9545)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_32323548 || tensor(0.9622) || tensor(0.8936) || tensor(0.9631)\n",
      "new_finetuning_all_0.5_img_2500_0.0006_55168461 || tensor(0.9594) || tensor(0.8898) || tensor(0.9601)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name in sorted([n['model'] for n in to_save_img_same]):\n",
    "    for i in to_save_img_same:\n",
    "        if i['model']==name:\n",
    "            print(i['model'], '||', i['AUROC'],'||', i['ACC'], '||', i['AUPRC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd0a7cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.8267999887466431 || ACC: 0.7396830916404724 || AUPRC: 0.8283335566520691\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9549658894538879 || ACC: 0.8773702383041382 || AUPRC: 0.9565150141716003\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9570051431655884 || ACC: 0.8842871785163879 || AUPRC: 0.9580898284912109\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9525769352912903 || ACC: 0.8798591494560242 || AUPRC: 0.9534769058227539\n",
      "\n",
      "Average classification performance based on the 5 models\n",
      "AUROC: 0.9561964273452759 || ACC: 0.8865715861320496 || AUPRC: 0.9570118188858032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in sorted(set([n['block'] for n in to_save_img_same])):\n",
    "    AUROC_mean = 0\n",
    "    ACC_mean = 0\n",
    "    AUPRC_mean = 0 \n",
    "    \n",
    "    for i in to_save_img_same:\n",
    "        if i['block']==b:\n",
    "            AUROC_mean += i['AUROC']\n",
    "            ACC_mean += i['ACC']\n",
    "            AUPRC_mean += i['AUPRC']\n",
    "    \n",
    "    print(f'Average classification performance based on the 5 models')\n",
    "    print(f'AUROC: {AUROC_mean/5} || ACC: {ACC_mean/5} || AUPRC: {AUPRC_mean/5}' )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7653fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
