{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3615de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import transform\n",
    "from skimage.transform import resize\n",
    "import skimage.exposure as skie\n",
    "\n",
    "import ot\n",
    "\n",
    "import torch\n",
    "from torch import manual_seed\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "from captum.attr import IntegratedGradients, Saliency, DeepLift, DeepLiftShap, GradientShap, GuidedBackprop,  GuidedGradCam \n",
    "from captum.attr import Deconvolution, ShapleyValueSampling, Lime, KernelShap, LRP, InputXGradient, FeatureAblation\n",
    "from scipy.ndimage import sobel, laplace\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.set_num_threads(3)\n",
    "\n",
    "def compactness(blob_labels):\n",
    "    import math\n",
    "    compactness=[]\n",
    "    region=regionprops(blob_labels)\n",
    "    for rp in region:\n",
    "        area=rp.area\n",
    "        #perimeter=rp.perimeter\n",
    "        perimeter=rp.perimeter_crofton\n",
    "        c=(4*math.pi*area)/(perimeter**2)\n",
    "        compactness.append(c)\n",
    "    return compactness\n",
    "\n",
    "def get_blobs(image,lesion_size):\n",
    "    \"\"\"\n",
    "    gets the noise image after pre processing and returns blobs with the size equal to lesion size\n",
    "    image is the noise image \n",
    "    lesion_size can be an int\n",
    "    \"\"\"\n",
    "    \n",
    "    labeled, nr_objects = ndimage.label(image)\n",
    "    sizes = ndimage.sum_labels(image,labeled,range(nr_objects+1))    \n",
    "    mask_size=sizes!=lesion_size\n",
    "    small_blobs=labeled.copy()\n",
    "    remove_pixel = mask_size[small_blobs]\n",
    "    small_blobs[remove_pixel] = 0 \n",
    "    \n",
    "    return small_blobs\n",
    "\n",
    "\n",
    "def list_lesions(image,lesion_size):\n",
    "    round_lesion_c=0.8 #lesions with compactness above this value are considered round\n",
    "    not_round_lesion_c=0.4 #lesions with compactness bellow this value are considered not round\n",
    "    \n",
    "    border=2\n",
    "    blur=0.75\n",
    "    \n",
    "    less_size=lesion_size*0.05*blur #the lesion is smaller after the blur\n",
    "    \n",
    "    small_blobs=get_blobs(image,lesion_size)\n",
    "    rb=regionprops(small_blobs)\n",
    "    round_lesions=[]\n",
    "    not_round_lesions=[]\n",
    "    c_list=compactness(small_blobs)\n",
    "    \n",
    "    for blob in range(len(c_list)):\n",
    "        \n",
    "        if c_list[blob]>round_lesion_c:#round lesions\n",
    "\n",
    "            blob_img=rb[blob].image.astype(float)\n",
    "            #the image is padded because when it is smoothed it increases a bit\n",
    "            pad_img=np.pad(array=blob_img, pad_width=border, mode='constant', constant_values=0)\n",
    "            blur_image=ndimage.gaussian_filter(pad_img, blur)\n",
    "            blur_image[blur_image<0.2]=0\n",
    "            energy=round(np.sum(blur_image.astype(float)),2)\n",
    "            if energy<=lesion_size-less_size+1 and energy>=lesion_size-less_size-1:\n",
    "                round_lesions.append([blur_image,round(c_list[blob],3)])\n",
    "        \n",
    "        \n",
    "        elif c_list[blob]<not_round_lesion_c:# not round lesions\n",
    "            blob_img=rb[blob].image.astype(float)\n",
    "            #the image is padded because when it is smoothed it increases a bit\n",
    "            pad_img=np.pad(array=blob_img, pad_width=border, mode='constant', constant_values=0)\n",
    "            blur_image=ndimage.gaussian_filter(pad_img, blur)\n",
    "            blur_image[blur_image<0.15]=0\n",
    "            energy=round(np.sum(blur_image.astype(float)),2)\n",
    "            if energy<=lesion_size-less_size+1 and energy>=lesion_size-less_size-1:\n",
    "                not_round_lesions.append([blur_image,round(c_list[blob],3)])\n",
    "            \n",
    "    return round_lesions, not_round_lesions\n",
    "\n",
    "def create_lesions(lesion_number,lesion_size,factor=1):\n",
    "    #factor is the value for which we multiply the sides of the lesion\n",
    "    round_lesions=[]\n",
    "    not_round_lesions=[]\n",
    "    s=0\n",
    "    size_noise=256\n",
    "    blur_radius=2\n",
    "\n",
    "    while len(not_round_lesions)<=lesion_number or len(round_lesions)<=lesion_number:\n",
    "        #create noise\n",
    "        np.random.seed(s)\n",
    "        noise_img=np.random.rand(size_noise,size_noise)\n",
    "\n",
    "        #smooth noise\n",
    "        imgf=ndimage.gaussian_filter(noise_img, blur_radius)\n",
    "\n",
    "        #create binary image\n",
    "        thr=threshold_otsu(imgf)\n",
    "        imgf_bin=imgf>thr\n",
    "        \n",
    "        #morphologic changes\n",
    "        erosion_image=ndimage.binary_erosion(imgf_bin)\n",
    "        open_er_img=ndimage.binary_opening(erosion_image)\n",
    "        erosion_image2=ndimage.binary_erosion(open_er_img)\n",
    "\n",
    "        #images\n",
    "        \n",
    "\n",
    "        #as a result from one noise image we create several images that can be used to create the lesions\n",
    "        round_lesions_open, not_round_lesions_open=list_lesions(open_er_img,lesion_size)\n",
    "        round_lesions_er, not_round_lesions_er=list_lesions(erosion_image2,lesion_size)\n",
    "        \n",
    "        round_lesions=round_lesions+round_lesions_open+round_lesions_er\n",
    "        not_round_lesions=not_round_lesions+not_round_lesions_open+not_round_lesions_er\n",
    "        \n",
    "        \n",
    "        #print('len lists:',len(round_lesions),len(not_round_lesions))\n",
    "        '''        \n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.subplot(2,4,1)\n",
    "        plt.imshow(noise_img)\n",
    "        plt.title('noise')\n",
    "        plt.subplot(2,4,2)\n",
    "        plt.imshow(imgf)\n",
    "        plt.title('blur')\n",
    "        plt.subplot(2,4,3)\n",
    "        plt.imshow(imgf_bin)\n",
    "        plt.title('binary img')\n",
    "        plt.subplot(2,4,4)\n",
    "        plt.imshow(erosion_image)\n",
    "        plt.title('erosion')\n",
    "        plt.subplot(2,4,5)\n",
    "        plt.imshow(open_er_img)\n",
    "        plt.title('open')\n",
    "        plt.subplot(2,4,6)\n",
    "        plt.imshow(erosion_image2)\n",
    "        plt.title('erosion2')\n",
    "        plt.subplot(2,4,7)\n",
    "        plt.imshow(dilated_image)\n",
    "        plt.title('dilation')\n",
    "        break\n",
    "        '''\n",
    "        \n",
    "        s+=1\n",
    "    print(f'round lesions: {len(round_lesions)}/{lesion_number} === not round lesions {len(not_round_lesions)}/{lesion_number}')\n",
    "    print(f'number of seeds used: {s-1}')\n",
    "    print(f'the lesions have size between {lesion_size-lesion_size*0.05*0.75-1} and {lesion_size-lesion_size*0.05*0.75+1}')\n",
    "    \n",
    "    lesions_r=round_lesions[:lesion_number]\n",
    "    lesions_nr=not_round_lesions[:lesion_number]\n",
    "    round_lesions=[[np.array(resize(round_lesions[i][0],(round_lesions[i][0].shape[0]*factor,round_lesions[i][0].shape[1]*factor))),round_lesions[i][1]] for i in range(len(lesions_r))]\n",
    "    not_round_lesions=[[np.array(resize(not_round_lesions[i][0],(not_round_lesions[i][0].shape[0]*factor,not_round_lesions[i][0].shape[1]*factor))),not_round_lesions[i][1]] for i in range(len(lesions_nr))]\n",
    "    \n",
    "    return round_lesions, not_round_lesions\n",
    "\n",
    "def rescale_values(image,max_val,min_val):\n",
    "    '''\n",
    "    image - numpy array\n",
    "    max_val/min_val - float\n",
    "    '''\n",
    "    return (image-image.min())/(image.max()-image.min())*(max_val-min_val)+min_val\n",
    "\n",
    "def select_coordinates(slice_image, lesions,white_constant,seed):\n",
    "    '''\n",
    "    slice_image is the brain slice to use\n",
    "    lesions is a list of the lesions (with len=number_of_lesions) to use\n",
    "    colour_lesion is either 'black' or 'white'\n",
    "    white_constant is the constant that is multiplied with the lesion mask to create lighter or darker lesions\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    brain_mask=np.array(slice_image)>0\n",
    "    brain_mask=brain_mask.astype(float)\n",
    "    x,y = np.where(brain_mask == 1.)\n",
    "    \n",
    "    lesion_brain=slice_image.copy().astype(float)\n",
    "    lesion_mask=brain_mask.copy() \n",
    "    lesion_added=0\n",
    "    ground_truth=np.zeros(slice_image.shape)\n",
    "    min_value=0.1 #min value for the lesion intensity \n",
    "    max_value=0.9 #max value for the lesion intensity\n",
    "    brain_image=slice_image.copy()\n",
    "    \n",
    "    while lesion_added<len(lesions):\n",
    "        i=np.random.choice(np.arange(len(x)))\n",
    "        coordinate=[x[i],y[i]]\n",
    "        lesion=lesions[lesion_added]\n",
    "        lesion_rescale = rescale_values(lesion,max_value,min_value)\n",
    "        lesion_rescale=rescale_values(lesion,white_constant,min_value)\n",
    "        \n",
    "        #creating the lesion mask and ground truth\n",
    "        if (brain_mask[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]] ==1).all():\n",
    "            #checks if the lesion that will be added is completly in a white space of the lesion mask \n",
    "            #(this means that the new lesion is not overlaping an existing one and is completly in the brain area)\n",
    "            lesion_mask[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]] -= lesion_rescale\n",
    "            lesion_added+=1\n",
    "            ground_truth[coordinate[0]: coordinate[0] + lesion.shape[0], coordinate[1]: coordinate[1] + lesion.shape[1]]+=lesion\n",
    "            brain_mask=lesion_mask\n",
    "\n",
    "    \n",
    "    brain_image=slice_image.copy()\n",
    "    brain_mask=np.array(slice_image)>0\n",
    "    #creating the white lesions\n",
    "       \n",
    "    brain_image[brain_image>0]=1-brain_image[brain_image>0]        \n",
    "    brain_image[lesion_mask!=0]*=lesion_mask[lesion_mask!=0]\n",
    "    brain_image[brain_mask]=1-brain_image[brain_mask]\n",
    "    \n",
    "\n",
    "    \n",
    "    return lesion_mask,brain_image,ground_truth\n",
    "    \n",
    "def add_lesions(slice_image,round_lesions,not_round_lesions,min_lesion,max_lesion,white_constant,seed,max_brain):\n",
    "    #for each slice we chose: random number of lesions, random lesions, random coordinates\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    number_of_lesions=np.random.randint(min_lesion,max_lesion+1)\n",
    "    lesion_type=np.random.randint(0,2)\n",
    "    #lesion_type=0 - round\n",
    "    #lesion_type=1 - not round\n",
    "    \n",
    "    #get lesions from type of lesions (and target)\n",
    "    added=rescale_values(slice_image.copy(),max_brain,0)\n",
    "    if lesion_type==0: #round\n",
    "        target=0\n",
    "        with open('round_lesions.pkl', 'rb') as f:\n",
    "            lesion_list=pickle.load(f)\n",
    "        np.random.shuffle(round_lesions)\n",
    "        \n",
    "    elif lesion_type==1: #not round\n",
    "        target=1\n",
    "        with open('not_round_lesions.pkl', 'rb') as f:\n",
    "            lesion_list=pickle.load(f) \n",
    "        np.random.shuffle(lesion_list)\n",
    "        \n",
    "    lesions=[i[0] for i in lesion_list[:number_of_lesions]]\n",
    "    #add the lesions\n",
    "    \n",
    "    lesion_mask,lesion_brain_white,ground_truth=select_coordinates(added, lesions,white_constant,seed)\n",
    "    \n",
    "    \n",
    "    return lesion_mask,lesion_brain_white,ground_truth,target,number_of_lesions\n",
    "\n",
    "def change_images(image):\n",
    "    image=np.repeat(image[..., np.newaxis], 3, axis=2)\n",
    "    image=resize(image, (224, 224))\n",
    "    image=image.transpose(2,0,1)\n",
    "    return image\n",
    "\n",
    "def create_dataset(slices,round_lesions,not_round_lesions,min_lesion=3,max_lesion=5,white_constant=0.85,seed=0,max_brain=1):\n",
    "\n",
    "    dataset_white=[]\n",
    "    number_lesions=[]\n",
    "    lesion_mask_list=[]\n",
    "    ground_truths=[]\n",
    "    for slice_idx in range(len(slices)):\n",
    "        lesion_mask,lesion_brain_white,ground_truth,target,number_of_lesions=add_lesions(slices[slice_idx],\n",
    "                                                                                         round_lesions,\n",
    "                                                                                         not_round_lesions,\n",
    "                                                                                         min_lesion=min_lesion,\n",
    "                                                                                         max_lesion=max_lesion,\n",
    "                                                                                         white_constant=white_constant,\n",
    "                                                                                        seed=seed,\n",
    "                                                                                        max_brain=max_brain)\n",
    "        dataset_white.append([change_images(lesion_brain_white),target])\n",
    "        number_lesions.append(number_of_lesions)\n",
    "        lesion_mask_list.append(lesion_mask)\n",
    "        ground_truths.append(ground_truth)\n",
    "        seed+=1\n",
    "        \n",
    "        if slice_idx%1500==0:\n",
    "            print(f'slice {slice_idx}/{len(slices)} = {round(100*slice_idx/len(slices),2)}%')\n",
    "        \n",
    "    return dataset_white,number_lesions,lesion_mask_list,ground_truths\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf544c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolList2BinString(lst):\n",
    "    # lst is a binary list that corresponds to the comparison between the predicted labels and the real labels\n",
    "    # returns a binary string version of the list\n",
    "    \n",
    "    return '0b' + ''.join(['1' if x else '0' for x in lst])\n",
    "\n",
    "\n",
    "def correctly_classified_intersection(to_save_list):\n",
    "    # to_save_list is a dictionary with the keys: \n",
    "    #    model: name of the file where the model is saved\n",
    "    #    AUROC: AUROC of the correspondig model\n",
    "    #    ACC: Accuracy of the correspondig model\n",
    "    #    AUPRC: AUPRC of the correspondig model\n",
    "    #    real_labels: tensor with the real labels\n",
    "    #    pred_labels: list of the predicted labels\n",
    "    #    logits: tensor with the logits predicted by the model\n",
    "    #    block: degree of finetuning (1, 2, 3, 4 or a)\n",
    "    #    seed: the seed used to train the model\n",
    "    #\n",
    "    # returns the indexes of the images that are correctly classified by every model\n",
    "    # (the dataset of these images should be the one used to obtain the labels and logits of to_save_list)\n",
    "\n",
    "    bools = []\n",
    "    for i in to_save_list:\n",
    "        compare_list = np.array(i['real_labels'])==np.array(i['pred_labels'])\n",
    "        bins = int(boolList2BinString(compare_list),2)\n",
    "        bools.append(bins)\n",
    "\n",
    "    #obtain the intersection of the correctly classified values using bitwize and between the bins and a list of ones \n",
    "    value = '1'\n",
    "    l=[str(value) for _ in range(len(dataset))]\n",
    "    res=int('0b'+''.join(l),2)\n",
    "    for i in bools:\n",
    "        res = res & i #bitwise and\n",
    "\n",
    "    # calculate the number of correct \n",
    "   \n",
    "    r='{0:08319b}'.format(res)\n",
    "    n=0\n",
    "    for i in r:\n",
    "        if i=='1':\n",
    "            n+=1\n",
    "    \n",
    "    print(f\"Obtained {n} images correctly classified by all the models in {len('{0:08319b}'.format(res))} total images\")\n",
    "\n",
    "    idx=[i for i,x in enumerate(r) if x=='1']\n",
    "    \n",
    "        \n",
    "    return idx\n",
    "    \n",
    "    \n",
    "    \n",
    "def incorrectly_classified_intersection(to_save_list):\n",
    "    # to_save_list is a dictionary with the keys: \n",
    "    #    model: name of the file where the model is saved\n",
    "    #    AUROC: AUROC of the correspondig model\n",
    "    #    ACC: Accuracy of the correspondig model\n",
    "    #    AUPRC: AUPRC of the correspondig model\n",
    "    #    real_labels: tensor with the real labels\n",
    "    #    pred_labels: list of the predicted labels\n",
    "    #    logits: tensor with the logits predicted by the model\n",
    "    #    block: degree of finetuning (1, 2, 3, 4 or a)\n",
    "    #    seed: the seed used to train the model\n",
    "    #\n",
    "    # returns the indexes of the images that are correctly classified by every model\n",
    "    # (the dataset of these images should be the one used to obtain the labels and logits of to_save_list)\n",
    "\n",
    "    bools = []\n",
    "    for i in to_save_list:\n",
    "        compare_list = np.array(i['real_labels'])==np.array(i['pred_labels'])\n",
    "        bins = int(boolList2BinString(compare_list),2)\n",
    "        bools.append(bins)\n",
    "\n",
    "    #obtain the intersection of the correctly classified values using bitwize or between the bins and a list of zeros \n",
    "    value = '0'\n",
    "    l=[str(value) for _ in range(len(dataset))]\n",
    "    res=int('0b'+''.join(l),2)\n",
    "    for i in bools:\n",
    "        res = res | i #bitwise or\n",
    "\n",
    "    # calculate the number of correct \n",
    "    r='{0:08319b}'.format(res)\n",
    "    n=0\n",
    "    for i in r:\n",
    "        if i=='0':\n",
    "            n+=1\n",
    "            \n",
    "    print(f\"Obtained {n} images incorrectly classified by all the models in {len('{0:08319b}'.format(res))} total images\")\n",
    "\n",
    "    idx=[i for i,x in enumerate(r) if x=='0']\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18970268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VGG_model(path,device):\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.classifier=model.classifier[:-1]\n",
    "    last_layers=[nn.Linear(4096,2)]\n",
    "    model.classifier = nn.Sequential(*list(model.classifier)+last_layers) \n",
    "\n",
    "    model.load_state_dict(torch.load(path,map_location=device))\n",
    "    \n",
    "    model.features[1]=nn.ReLU(inplace=False)\n",
    "    model.features[3]=nn.ReLU(inplace=False)\n",
    "    model.features[6]=nn.ReLU(inplace=False)\n",
    "    model.features[8]=nn.ReLU(inplace=False)\n",
    "    model.features[11]=nn.ReLU(inplace=False)\n",
    "    model.features[13]=nn.ReLU(inplace=False)\n",
    "    model.features[15]=nn.ReLU(inplace=False)\n",
    "    model.features[18]=nn.ReLU(inplace=False)\n",
    "    model.features[20]=nn.ReLU(inplace=False)\n",
    "    model.features[22]=nn.ReLU(inplace=False)\n",
    "    model.features[25]=nn.ReLU(inplace=False)\n",
    "    model.features[27]=nn.ReLU(inplace=False)\n",
    "    model.features[29]=nn.ReLU(inplace=False)\n",
    "    model.classifier[1]=nn.ReLU(inplace=False)\n",
    "    model.classifier[4]=nn.ReLU(inplace=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f30812a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanations(model, dataset_img, gt_image, DEVICE):\n",
    "    # model is the VGG model\n",
    "    # dataset img is an element of dataset (image,target)\n",
    "    # gt image is a 224x224 gt image\n",
    "    test_img, target = dataset_img\n",
    "    \n",
    "    transformed_img=torch.tensor(test_img)\n",
    "    input_image = transformed_img.unsqueeze(0)\n",
    "    data = input_image.to(DEVICE,dtype=torch.float)\n",
    "\n",
    "\n",
    "    attribution_gradient = abs(IntegratedGradients(model).attribute(data, target=target).cpu().detach().numpy())\n",
    "    attribution_gradientshap = abs(GradientShap(model).attribute(data, target=target, baselines=torch.zeros(data.shape).to(DEVICE)).cpu().detach().numpy())\n",
    "    attribution_deeplift = abs(DeepLift(model).attribute(data, target=target).cpu().detach().numpy())\n",
    "    attribution_saliency = abs(np.array(torch.Tensor.cpu(Saliency(model).attribute(data, target=target))))\n",
    "    attribution_InputXGradient=abs(InputXGradient(model).attribute(data, target=target).cpu().detach().numpy())\n",
    "    attribution_backprop = abs(np.array(torch.Tensor.cpu(GuidedBackprop(model).attribute(data, target=target))))\n",
    "    attribution_deconv = abs(Deconvolution(model).attribute(data, target=target).cpu().detach().numpy())\n",
    "    attribution_LRP = abs(LRP(model).attribute(data, target=target).cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    gt=np.dot(gt_image[...,:3], rgb_weights) \n",
    "    \n",
    "    return attribution_gradient, attribution_gradientshap, attribution_deeplift, attribution_saliency, attribution_InputXGradient, attribution_backprop, attribution_deconv, attribution_LRP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c78efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_derivations_from_confusion_matrix(exp,gt):\n",
    "    # obtains the tp, tn, fp and fn top p pixels with the highest intensity in exp, where p is the number of pixels !=0 in gt.\n",
    "    \n",
    "    p = len(gt[gt>0])\n",
    "    \n",
    "    # turn exp into grayscale\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    att=exp.squeeze(0).transpose(1,2,0)    \n",
    "    grayscale_att = np.dot(att[...,:3], rgb_weights)\n",
    "    \n",
    "    # create a mask of the pixels with the highest intensity\n",
    "    sorted_=np.sort(grayscale_att, axis=None)\n",
    "    sorted_=sorted_[::-1]\n",
    "    min_att_val=sorted_[p]\n",
    "\n",
    "    mask=grayscale_att>min_att_val\n",
    "\n",
    "    tp = mask*(gt>0)\n",
    "    tp = len(tp[tp > 0])\n",
    "    \n",
    "    tn = ~mask*~(gt>0)\n",
    "    tn = len(tn[tn>0])\n",
    "    \n",
    "    fp = mask*~(gt>0)\n",
    "    fp = len(fp[fp>0])\n",
    "    \n",
    "    fn = ~mask*(gt>0)\n",
    "    fn = len(fn[fn>0])\n",
    "\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a1c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "\n",
    "def metrics_per_model(model,image_dataset,ground_truths_dataset,device):\n",
    "    t0 = time.time()\n",
    "    metrics = {\n",
    "               'gradient': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'gradshap': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'deeplift': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'saliency': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'inputXGrad': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'backprop': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'deconv': {'TP':[],'TN':[], 'FN':[], 'FP':[]},\n",
    "               'LRP': {'TP':[],'TN':[], 'FN':[], 'FP':[]}\n",
    "              }\n",
    "\n",
    "    xai_methods = ['gradient', 'gradshap', 'deeplift', 'saliency', 'inputXGrad', 'backprop', 'deconv', 'LRP']\n",
    "\n",
    "    for image_idx in range(len(image_dataset)):\n",
    "        gt = ground_truths_dataset[image_idx]\n",
    "        #obtain explanation\n",
    "        explanations_heatmaps = explanations(model,image_dataset[image_idx],gt,device)\n",
    "        \n",
    "        for i, exp in enumerate(explanations_heatmaps):\n",
    "            \n",
    "            tp, tn, fp, fn = obtain_derivations_from_confusion_matrix(exp,gt)\n",
    "            \n",
    "            metrics[xai_methods[i]]['TP'].append(tp)\n",
    "            metrics[xai_methods[i]]['TN'].append(tn)\n",
    "            metrics[xai_methods[i]]['FP'].append(fp)\n",
    "            metrics[xai_methods[i]]['FN'].append(fn)\n",
    "            \n",
    "        if image_idx%500 == 0:\n",
    "            print(f'{image_idx}/{len(image_dataset)} | {100*round(image_idx/len(image_dataset),2)} %')\n",
    "            print(f'time elapsed: {(time.time()-t0) // 60:.0f}m {(time.time()-t0) % 60:.0f}s')\n",
    "            \n",
    "            \n",
    "    return metrics\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a7d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mcuda01                       \u001b[m  Wed Mar 29 11:30:49 2023  \u001b[1m\u001b[30m470.161.03\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 55'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7955\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m803M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1875M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m5267M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 68'C\u001b[m, \u001b[32m 11 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7823\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m613M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1905M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m5271M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 58'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 6531\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1947M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1947M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m2627M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 61'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3904\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1947M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1947M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 63'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 9439\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2307M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1851M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m5271M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 55'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 2637\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m2629M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 66'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 5279\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m5271M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 32'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ded96a",
   "metadata": {},
   "source": [
    "### Obtaining the intersection of the correctly classified and respective ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8623d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round lesions: 51/50 === not round lesions 207/50\n",
      "number of seeds used: 886\n",
      "the lesions have size between 66.375 and 68.375\n",
      "took 12.68s\n",
      " ====== holdout ====== \n",
      "slice 0/8539 = 0.0%\n",
      "slice 1500/8539 = 17.57%\n",
      "slice 3000/8539 = 35.13%\n",
      "slice 4500/8539 = 52.7%\n",
      "slice 6000/8539 = 70.27%\n",
      "slice 7500/8539 = 87.83%\n",
      "\n",
      "took 261.98s\n",
      "4277 slices of target 1 out of 8539 slices: 50.09 %\n",
      " number of slices: 8539\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "plt.rc('image',cmap='gray')  \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# creating lesions\n",
    "number_of_lesions=50 #amount of lesions in each lesion list\n",
    "size_of_lesions=70 #size of all the lesions\n",
    "factor=2\n",
    "\n",
    "round_lesions, not_round_lesions=create_lesions(number_of_lesions,size_of_lesions,factor=factor)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(f'took {round(elapsed,2)}s')\n",
    "\n",
    "# loading slices \n",
    "with open('slices_validation.pkl', 'rb') as f:\n",
    "    validation_slices,target_valid_gender,target_valid_age = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# adding lesions to slices\n",
    "lesion_max_intensity=0.5\n",
    "max_brain_intensity=0.7\n",
    "\n",
    "start = time.time()\n",
    "print(' ====== holdout ====== ')\n",
    "\n",
    "dataset,_,_,ground_truths=create_dataset(validation_slices,\n",
    "                                          round_lesions,\n",
    "                                          not_round_lesions,\n",
    "                                          min_lesion=3,\n",
    "                                          max_lesion=5,\n",
    "                                          white_constant=lesion_max_intensity,\n",
    "                                          seed=50000,\n",
    "                                          max_brain=max_brain_intensity)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print()\n",
    "print(f'took {round(elapsed,2)}s')\n",
    "target_w=[i[1] for i in dataset]\n",
    "ground_truths = [resize(gt, (224, 224)) for gt in ground_truths]\n",
    "\n",
    "print(f'{len([i for i in target_w if i==1])} slices of target 1 out of {len(target_w)} slices: {round(100*len([i for i in target_w if i==1])/len(target_w),2)} %')\n",
    "print(f' number of slices: {len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764ab3f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 3681 images correctly classified by all the models in 8539 total images\n",
      "\n",
      "new_finetuning_3conv_0-Copy1.5_img_2500_0.008_116560000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martao/anaconda3/lib/python3.9/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/martao/anaconda3/lib/python3.9/site-packages/captum/attr/_core/deep_lift.py:336: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n",
      "/home/martao/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:749: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "/home/martao/anaconda3/lib/python3.9/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:64: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 22s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 47s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 7s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 32s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 26m 56s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 32m 18s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 37m 39s\n",
      "\n",
      "new_finetuning_all_0-Copy1.5_img_2500_0.004_116560000.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 20s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 38s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 15m 58s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 19s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 26m 39s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 31m 59s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 37m 20s\n",
      "\n",
      "new_finetuning_2conv_0.5_MRI_new_2500_0.03_18464.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 21s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 40s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 0s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 22s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 26m 45s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 32m 9s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 37m 36s\n",
      "\n",
      "new_finetuning_all_0.5_MRI_new_2500_0.01_55168461.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 24s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 47s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 13s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 39s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 27m 7s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 32m 33s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 37m 60s\n",
      "\n",
      "new_finetuning_all_0-Copy1.5_img_2500_0.004_58461.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 26s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 52s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 17s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 43s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 27m 9s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 32m 35s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 38m 1s\n",
      "\n",
      "new_finetuning_1conv_0-Copy1.5_img_2500_0.02_646976.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 26s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 53s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 22s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 52s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 27m 20s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 32m 48s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 38m 15s\n",
      "\n",
      "new_finetuning_3conv_0-Copy1.5_img_2500_0.008_98794515.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 27s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 10m 55s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 24s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 21m 56s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 27m 27s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 32m 59s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 38m 34s\n",
      "\n",
      "new_finetuning_1conv_0-Copy1.5_img_2500_0.02_23548.pt\n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 5m 33s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 11m 5s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 16m 36s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 22m 7s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 27m 38s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 33m 7s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 38m 34s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/best/save/to_save.pkl', 'rb') as f:\n",
    "    to_save_best_img = pickle.load(f)\n",
    "    \n",
    "with open('/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/best/save/metrics_best.pkl', 'rb') as f:\n",
    "    metrics_best = pickle.load(f)\n",
    "    \n",
    "folder = '/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/best/'\n",
    "\n",
    "models_names = os.listdir(folder)\n",
    "models_names = [i for i in models_names if (i[0]=='n' and i not in list(metrics_best.keys()))]\n",
    "\n",
    "# correctly classified intersection\n",
    "idx = correctly_classified_intersection(to_save_best_img)\n",
    "\n",
    "correct_best=[dataset[i] for i in idx]\n",
    "correct_gt_best=[ground_truths[i] for i in idx]\n",
    "\n",
    "\n",
    "device = 'cuda:7'\n",
    "print()\n",
    "\n",
    "for model_name in models_names:\n",
    "    print(model_name)\n",
    "    model = load_VGG_model(folder+model_name,device)\n",
    "    metrics_per_model_dict = metrics_per_model(model,correct_best,correct_gt_best,device)\n",
    "    metrics_best[model_name] = metrics_per_model_dict\n",
    "    \n",
    "    with open(folder+'save/metrics_best.pkl', 'wb') as f:\n",
    "        pickle.dump(metrics_best,f)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfaed469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 3350 images correctly classified by all the models in 8539 total images\n",
      "\n",
      "new_finetuning_3conv_0.5_MRI_new_2500_0.005_55168461.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martao/anaconda3/lib/python3.9/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/martao/anaconda3/lib/python3.9/site-packages/captum/attr/_core/deep_lift.py:336: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n",
      "/home/martao/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:749: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "/home/martao/anaconda3/lib/python3.9/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:64: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3350 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3350 | 15.0 %\n",
      "time elapsed: 5m 32s\n",
      "1000/3350 | 30.0 %\n",
      "time elapsed: 11m 5s\n",
      "1500/3350 | 45.0 %\n",
      "time elapsed: 16m 39s\n",
      "2000/3350 | 60.0 %\n",
      "time elapsed: 22m 14s\n",
      "2500/3350 | 75.0 %\n",
      "time elapsed: 27m 46s\n",
      "3000/3350 | 90.0 %\n",
      "time elapsed: 33m 18s\n",
      "\n",
      "new_finetuning_2conv_0-Copy1.5_img_2500_0.0018_5484646.pt\n",
      "0/3350 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3350 | 15.0 %\n",
      "time elapsed: 5m 34s\n",
      "1000/3350 | 30.0 %\n",
      "time elapsed: 11m 7s\n",
      "1500/3350 | 45.0 %\n",
      "time elapsed: 16m 38s\n",
      "2000/3350 | 60.0 %\n",
      "time elapsed: 22m 5s\n",
      "2500/3350 | 75.0 %\n",
      "time elapsed: 27m 36s\n",
      "3000/3350 | 90.0 %\n",
      "time elapsed: 33m 2s\n",
      "\n",
      "new_finetuning_3conv_0-Copy1.5_img_2500_0.0008_18464876.pt\n",
      "0/3350 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3350 | 15.0 %\n",
      "time elapsed: 5m 29s\n",
      "1000/3350 | 30.0 %\n",
      "time elapsed: 10m 59s\n",
      "1500/3350 | 45.0 %\n",
      "time elapsed: 16m 28s\n",
      "2000/3350 | 60.0 %\n",
      "time elapsed: 21m 57s\n",
      "2500/3350 | 75.0 %\n",
      "time elapsed: 27m 28s\n",
      "3000/3350 | 90.0 %\n",
      "time elapsed: 33m 1s\n",
      "\n",
      "new_finetuning_2conv_0.5_MRI_new_2500_0.01_116560000.pt\n",
      "0/3350 | 0.0 %\n",
      "time elapsed: 0m 1s\n",
      "500/3350 | 15.0 %\n",
      "time elapsed: 5m 33s\n",
      "1000/3350 | 30.0 %\n",
      "time elapsed: 11m 5s\n",
      "1500/3350 | 45.0 %\n",
      "time elapsed: 16m 39s\n",
      "2000/3350 | 60.0 %\n",
      "time elapsed: 22m 11s\n",
      "2500/3350 | 75.0 %\n",
      "time elapsed: 27m 45s\n",
      "3000/3350 | 90.0 %\n",
      "time elapsed: 33m 19s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/same/save/to_save.pkl', 'rb') as f:\n",
    "    to_save_same_img = pickle.load(f)\n",
    "    \n",
    "with open('/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/same/save/metrics_same.pkl', 'rb') as f:\n",
    "    metrics_same = pickle.load(f)  \n",
    "\n",
    "folder = '/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/same/'\n",
    "models_names = os.listdir(folder)\n",
    "models_names = [i for i in models_names if (i[0]=='n' and i not in list(metrics_same.keys()))]\n",
    "\n",
    "# correctly classified intersection\n",
    "idx = correctly_classified_intersection(to_save_same_img)\n",
    "\n",
    "correct_same=[dataset[i] for i in idx]\n",
    "correct_gt_same=[ground_truths[i] for i in idx]\n",
    "\n",
    "device = 'cuda:7'\n",
    "print()\n",
    "\n",
    "for model_name in models_names:\n",
    "    print(model_name)\n",
    "    model = load_VGG_model(folder+model_name,device)\n",
    "    metrics_per_model_dict = metrics_per_model(model,correct_same,correct_gt_same,device)\n",
    "    metrics_same[model_name] = metrics_per_model_dict\n",
    "    \n",
    "    with open(folder+'save/metrics_same.pkl', 'wb') as f:\n",
    "        pickle.dump(metrics_same,f)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed47b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 3681 images correctly classified by all the models in 8539 total images\n",
      "Obtained 3350 images correctly classified by all the models in 8539 total images\n"
     ]
    }
   ],
   "source": [
    "folder_best = '/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/best/save/'\n",
    "\n",
    "with open(folder_best + 'to_save.pkl', 'rb') as f:\n",
    "    to_save_best_img = pickle.load(f)\n",
    "\n",
    "idx = correctly_classified_intersection(to_save_best_img)\n",
    "\n",
    "correct_best=[dataset[i] for i in idx]\n",
    "correct_gt_best=[ground_truths[i] for i in idx]\n",
    "\n",
    "folder_same = '/home/martao/MRI_dataset/2ndTry/new_models_finetune_with_test_slices/same/save/'\n",
    "\n",
    "with open(folder_same + 'to_save.pkl', 'rb') as f:\n",
    "    to_save_same_img = pickle.load(f)\n",
    "\n",
    "idx = correctly_classified_intersection(to_save_same_img)\n",
    "\n",
    "correct_same=[dataset[i] for i in idx]\n",
    "correct_gt_same=[ground_truths[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf567266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sobel and laplace\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "def obtain_edge_filters(dataset,ground_truth):\n",
    "    t0 = time.time()\n",
    "    laplace_metircs = {'TP':[],'TN':[], 'FN':[], 'FP':[]}\n",
    "    sobel_metircs = {'TP':[],'TN':[], 'FN':[], 'FP':[]}\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        image=dataset[i][0]\n",
    "        gt=ground_truth[i]\n",
    "        \n",
    "        expl_sobel = np.array([ndimage.sobel(image)])\n",
    "        expl_laplace = np.array([ndimage.laplace(image)])\n",
    "        \n",
    "        \n",
    "        metrics_sobel = obtain_derivations_from_confusion_matrix(expl_sobel, gt)\n",
    "        metrics_laplace = obtain_derivations_from_confusion_matrix(expl_laplace, gt)\n",
    "        \n",
    "        laplace_metircs['TP'].append(metrics_laplace[0])\n",
    "        laplace_metircs['TN'].append(metrics_laplace[1])\n",
    "        laplace_metircs['FP'].append(metrics_laplace[2])\n",
    "        laplace_metircs['FN'].append(metrics_laplace[3])\n",
    "        \n",
    "        sobel_metircs['TP'].append(metrics_laplace[0])\n",
    "        sobel_metircs['TN'].append(metrics_laplace[1])\n",
    "        sobel_metircs['FP'].append(metrics_laplace[2])\n",
    "        sobel_metircs['FN'].append(metrics_laplace[3])\n",
    "        \n",
    "        if i%500 == 0:\n",
    "            print(f'{i}/{len(dataset)} | {100*round(i/len(dataset),2)} %')\n",
    "            print(f'time elapsed: {(time.time()-t0) // 60:.0f}m {(time.time()-t0) % 60:.0f}s')\n",
    "        \n",
    "    return laplace_metircs, sobel_metircs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6223fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====== SAME PERFORMANCE ====== \n",
      "0/3350 | 0.0 %\n",
      "time elapsed: 0m 0s\n",
      "500/3350 | 15.0 %\n",
      "time elapsed: 0m 10s\n",
      "1000/3350 | 30.0 %\n",
      "time elapsed: 0m 19s\n",
      "1500/3350 | 45.0 %\n",
      "time elapsed: 0m 29s\n",
      "2000/3350 | 60.0 %\n",
      "time elapsed: 0m 39s\n",
      "2500/3350 | 75.0 %\n",
      "time elapsed: 0m 49s\n",
      "3000/3350 | 90.0 %\n",
      "time elapsed: 0m 58s\n",
      " ====== BEST PERFORMANCE ====== \n",
      "0/3681 | 0.0 %\n",
      "time elapsed: 0m 0s\n",
      "500/3681 | 14.000000000000002 %\n",
      "time elapsed: 0m 10s\n",
      "1000/3681 | 27.0 %\n",
      "time elapsed: 0m 20s\n",
      "1500/3681 | 41.0 %\n",
      "time elapsed: 0m 29s\n",
      "2000/3681 | 54.0 %\n",
      "time elapsed: 0m 39s\n",
      "2500/3681 | 68.0 %\n",
      "time elapsed: 0m 49s\n",
      "3000/3681 | 81.0 %\n",
      "time elapsed: 0m 59s\n",
      "3500/3681 | 95.0 %\n",
      "time elapsed: 1m 8s\n"
     ]
    }
   ],
   "source": [
    "print(' ====== SAME PERFORMANCE ====== ')\n",
    "laplace_same, sobel_same = obtain_edge_filters(correct_same, correct_gt_same)\n",
    "\n",
    "with open(folder_same + 'laplace.pkl', 'wb') as f:\n",
    "    pickle.dump(laplace_same,f)\n",
    "with open(folder_same + 'sobel.pkl', 'wb') as f:\n",
    "    pickle.dump(sobel_same,f)\n",
    "\n",
    "print(' ====== BEST PERFORMANCE ====== ')\n",
    "laplace_best, sobel_best = obtain_edge_filters(correct_best, correct_gt_best)\n",
    "\n",
    "with open(folder_best + 'laplace.pkl', 'wb') as f:\n",
    "    pickle.dump(laplace_best,f)\n",
    "with open(folder_best + 'sobel.pkl', 'wb') as f:\n",
    "    pickle.dump(sobel_best,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad7311d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mcuda01                       \u001b[m  Wed Mar 29 15:49:25 2023  \u001b[1m\u001b[30m470.161.03\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 52'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 6930\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1875M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m571M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m499M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m3975M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 58'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 8620\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1905M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m6681M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 54'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 59'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 6687\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m6679M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 58'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 5836\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m1851M\u001b[m) \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m3975M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 50'C\u001b[m, \u001b[1m\u001b[32m 48 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 2835\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m2827M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 60'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 2639\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mjohannes\u001b[m(\u001b[33m2631M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 46'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m11033\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mmartao\u001b[m(\u001b[33m11025M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0a9adf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random model\n",
    "device = 'cuda:2'\n",
    "random_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a84260b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(seed)\n",
    "random_model = models.vgg16(pretrained=False)\n",
    "random_model.classifier=random_model.classifier[:-1]\n",
    "last_layers=[nn.Linear(4096,2)]\n",
    "random_model.classifier = nn.Sequential(*list(random_model.classifier)+last_layers) \n",
    "\n",
    "random_model.to(device)\n",
    "\n",
    "random_model.features[1]=nn.ReLU(inplace=False)\n",
    "random_model.features[3]=nn.ReLU(inplace=False)\n",
    "random_model.features[6]=nn.ReLU(inplace=False)\n",
    "random_model.features[8]=nn.ReLU(inplace=False)\n",
    "random_model.features[11]=nn.ReLU(inplace=False)\n",
    "random_model.features[13]=nn.ReLU(inplace=False)\n",
    "random_model.features[15]=nn.ReLU(inplace=False)\n",
    "random_model.features[18]=nn.ReLU(inplace=False)\n",
    "random_model.features[20]=nn.ReLU(inplace=False)\n",
    "random_model.features[22]=nn.ReLU(inplace=False)\n",
    "random_model.features[25]=nn.ReLU(inplace=False)\n",
    "random_model.features[27]=nn.ReLU(inplace=False)\n",
    "random_model.features[29]=nn.ReLU(inplace=False)\n",
    "random_model.classifier[1]=nn.ReLU(inplace=False)\n",
    "random_model.classifier[4]=nn.ReLU(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7c351",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3350 | 0.0 %\n",
      "time elapsed: 0m 1s\n"
     ]
    }
   ],
   "source": [
    "same_random_explanations = metrics_per_model(random_model,correct_same,correct_gt_same,device)\n",
    "\n",
    "with open(folder_same + 'same_random_explanations.pkl', 'wb') as f:\n",
    "    pickle.dump(same_random_explanations,f)\n",
    "\n",
    "best_random_explanations = metrics_per_model(random_model,correct_best,correct_gt_best,device)\n",
    "\n",
    "\n",
    "with open(folder_best + 'best_random_explanations.pkl', 'wb') as f:\n",
    "    pickle.dump(best_random_explanations,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01e4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
